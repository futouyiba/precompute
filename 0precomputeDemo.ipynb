{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129c534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6baea6",
   "metadata": {},
   "source": [
    "### 链条式找配置，并组装紧密数组（每张地图只需做一次，可用于生成一串派生环境场）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "数据处理步骤 (Metaphysical Data Processing Steps):\n",
    "\n",
    "1.  **数据源与目标确立 (Source & Target Definition)**\n",
    "    - Source: 位于 `precompute/data` 下的静态配置表 (JSON)，定义了游戏世界的逻辑结构。\n",
    "    - Target: 位于 `ExportedData` 下的 Numpy 数组，代表了具体场景的物理数据 (Voxel/Grid)。\n",
    "\n",
    "2.  **标识符提取 (Identifier Extraction)**\n",
    "    - 从物理数据的文件名/路径中提取关键标识符 (SceneID / AssetID)。\n",
    "    - 例如: 从 `Fishing_1006001_Global.npy` 提取 `1006001`。\n",
    "\n",
    "3.  **逻辑映射建立 (Logical Mapping)**\n",
    "    - 利用中间件/配置表 (`map_scene.json`) 将物理标识符 (AssetID) 映射回系统逻辑标识符 (MapID)。\n",
    "    - 这一步是连接“即时演算数据”与“策划配置数据”的关键桥梁。\n",
    "\n",
    "4.  **上下文关联与整合 (Context Association & Integration)**\n",
    "    - 以 MapID 为锚点，级联查询关联的 Pond, Stock, Fish 配置。\n",
    "    - 将分散的数据整合为适合并行计算的结构化格式 (Numpy/Pandas)。\n",
    "\"\"\"\n",
    "\n",
    "# 配置路径\n",
    "DATA_ROOT = Path(r'D:\\fishinggame\\precompute\\data\\1\\1001')\n",
    "EXPORTED_DATA_ROOT = Path(r'D:\\fishinggame\\ExportedData')\n",
    "\n",
    "# 加载 map_scene.json\n",
    "with open(DATA_ROOT / 'map_scene.json', 'r', encoding='utf-8') as f:\n",
    "    map_scene = json.load(f)\n",
    "\n",
    "# 建立 assetId -> map_id 的反向索引\n",
    "asset_to_map = {info['assetId']: int(map_id) for map_id, info in map_scene.items() if info.get('assetId')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scene_id_lookup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件路径: D:\\fishinggame\\ExportedData\\Fishing_1006001_Dense_20260107_154037\\Fishing_1006001_Global.npy\n",
      "提取的 scene_id: 1006001\n",
      "对应的 map_id: 1009\n",
      "地图信息: {'id': 1009, 'name': 'map_base_6', 'desc': 106, 'assetId': '1006001', 'originOffsetX': 0, 'originOffsetY': 0, 'offsetX': 0, 'offsetY': 0, 'sizeX': 1000, 'sizeY': 1000, 'rotate': 0, 'mark': ''}\n"
     ]
    }
   ],
   "source": [
    "def get_scene_id_from_path(npy_path: str) -> str:\n",
    "    \"\"\"从npy文件路径中提取scene_id (如 Fishing_1006001_Global.npy -> '1006001')\"\"\"\n",
    "    match = re.search(r'Fishing_(\\d+)', str(npy_path))\n",
    "    if not match:\n",
    "        raise ValueError(f'无法从路径中提取scene_id: {npy_path}')\n",
    "    return match.group(1)\n",
    "\n",
    "def get_map_id_from_scene_id(scene_id: str) -> int:\n",
    "    \"\"\"根据scene_id查找对应的map_id\"\"\"\n",
    "    if scene_id in asset_to_map:\n",
    "        return asset_to_map[scene_id]\n",
    "    raise ValueError(f'找不到scene_id {scene_id} 对应的map_id')\n",
    "\n",
    "def get_map_id_from_npy_path(npy_path: str) -> int:\n",
    "    \"\"\"从npy文件路径直接获取map_id\"\"\"\n",
    "    scene_id = get_scene_id_from_path(npy_path)\n",
    "    return get_map_id_from_scene_id(scene_id)\n",
    "\n",
    "# 测试示例\n",
    "test_path = r'D:\\fishinggame\\ExportedData\\Fishing_1006001_Dense_20260107_154037\\Fishing_1006001_Global.npy'\n",
    "scene_id = get_scene_id_from_path(test_path)\n",
    "map_id = get_map_id_from_scene_id(scene_id)\n",
    "print(f'文件路径: {test_path}')\n",
    "print(f'提取的 scene_id: {scene_id}')\n",
    "print(f'对应的 map_id: {map_id}')\n",
    "print(f'地图信息: {map_scene[str(map_id)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a94c6",
   "metadata": {},
   "source": [
    "#### 对于局部池取stock\n",
    "* \n",
    "* 去D:\\fishinggame\\precompute\\data\\1\\1001\\fish_stock.json当中，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326ea7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 7 个鱼塘配置\n",
      "已加载 6 个 Stock 配置\n",
      "\n",
      "当前地图: map_base_6 (ID: 1009)\n",
      "关联的 Desc ID (用于对应 fish_pond_list.mapId): 106\n",
      "\n",
      "查找 mapId == 106 的鱼塘...\n",
      "找到 1 个关联鱼塘:\n",
      "  - Pond: Sunset_Stream (ID: 301020005) -> Stock ID: 301030106\n",
      "    Stock 详情: Name=stock_sunset, ResetTime=05:00\n"
     ]
    }
   ],
   "source": [
    "# 加载额外的配置表\n",
    "with open(DATA_ROOT / 'fish_pond_list.json', 'r', encoding='utf-8') as f:\n",
    "    fish_pond_list = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'fish_stock.json', 'r', encoding='utf-8') as f:\n",
    "    fish_stock_config = json.load(f)\n",
    "\n",
    "print(f\"已加载 {len(fish_pond_list)} 个鱼塘配置\")\n",
    "print(f\"已加载 {len(fish_stock_config)} 个 Stock 配置\")\n",
    "\n",
    "# 获取 map_id 对应的 map_scene 配置\n",
    "current_map_info = map_scene.get(str(map_id))\n",
    "if not current_map_info:\n",
    "    raise ValueError(f\"Found no map info for id {map_id}\")\n",
    "\n",
    "map_desc_id = current_map_info.get('desc')\n",
    "print(f\"\\n当前地图: {current_map_info['name']} (ID: {map_id})\")\n",
    "print(f\"关联的 Desc ID (用于对应 fish_pond_list.mapId): {map_desc_id}\")\n",
    "\n",
    "# 查找关联的 Pond 和 Stock\n",
    "print(f\"\\n查找 mapId == {map_desc_id} 的鱼塘...\")\n",
    "related_ponds = [pond for pond in fish_pond_list.values() if pond.get('mapId') == map_desc_id]\n",
    "\n",
    "if not related_ponds:\n",
    "    print(\"警告: 未找到关联的鱼塘配置 (Pond)\")\n",
    "else:\n",
    "    print(f\"找到 {len(related_ponds)} 个关联鱼塘:\")\n",
    "    for pond in related_ponds:\n",
    "        # 兼容处理: 有些json key可能是str类型的id\n",
    "        pond_id = pond.get('id')\n",
    "        pond_name = pond.get('name')\n",
    "        stock_id = pond.get('fishStockId')\n",
    "        \n",
    "        print(f\"  - Pond: {pond_name} (ID: {pond_id}) -> Stock ID: {stock_id}\")\n",
    "        \n",
    "        # 查询 Stock 详情 (注意 key 可能是字符串)\n",
    "        stock_info = fish_stock_config.get(str(stock_id))\n",
    "        if stock_info:\n",
    "             print(f\"    Stock 详情: Name={stock_info.get('name')}, ResetTime={stock_info.get('resetDayTime')}\")\n",
    "        else:\n",
    "             print(f\"    警告: 在 fish_stock.json 中未找到 Stock ID {stock_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a062d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顺着往下进行数据查找和组装numpy，供后面的计算使用。\n",
    "# 大致思路为：\n",
    "# 5. 遍历 Stock ID (Stock -> Release)：\n",
    "#    - 从相关联的池塘中提取 Stock ID，查找其下属的所有 Release ID。\n",
    "# 6. Eager Loading (Release -> Fish/Env):\n",
    "#    - 对每个 Release ID，立即提取所需的全部配置信息，包括：\n",
    "#      - 基础属性: qualityId (即原 fishId), weight/length ranges (min/max).\n",
    "#      - 关键系数: minEnvCoeff, minAdaptCoeff.\n",
    "#      - 关联元数据: speciesId, envAffinityId (即原 envId).\n",
    "# 7. 组装 DataFrame (Assembly):\n",
    "#    - 将上述所有提取的字段扁平化，组装成 Pandas DataFrame (`stockFishesPd`)。\n",
    "#    - 每一行代表一个 Release 配置，为后续的概率计算和环境场生成做准备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf363e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Release and Quality configs...\n",
      "Configs loaded.\n",
      "Processing 1 unique Stock IDs associated with the current map.\n",
      "Created stockFishesPd with 35 rows.\n",
      "     stockId  releaseId  qualityId  envAffinityId  speciesId  weight_min  weight_max  len_min  len_max  minEnvCoeff  minAdaptCoeff                                name  probWeight\n",
      "0  301030106     300500  101034430        1013390  101020063         150         450       26       37            0              0  Release_American_Shad_Young_sunset      250000\n",
      "1  301030106     300510  101034090        1013050  101020010          50         200       16       26            0              0    Release_Brook_Trout_Young_sunset      100000\n",
      "2  301030106     300520  101031007        1010066  101020010         200         350       26       32            0              0   Release_Brook_Trout_Common_sunset      100000\n",
      "3  301030106     300530  101034450        1013410  101020003         150         450       28       40            0              0         Release_Bowfin_Young_sunset      200000\n",
      "4  301030106     300540  101034510        1013470  101020050         550         700       37       40            0              0       Release_Hardhead_Young_sunset       17928\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load additional configurations\n",
    "print(\"Loading Release and Quality configs...\")\n",
    "with open(DATA_ROOT / 'stock_release.json', 'r', encoding='utf-8') as f:\n",
    "    stock_release_config = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'fish_release.json', 'r', encoding='utf-8') as f:\n",
    "    fish_release_config = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'basic_fish_quality.json', 'r', encoding='utf-8') as f:\n",
    "    basic_fish_quality_config = json.load(f)\n",
    "print(\"Configs loaded.\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "# 'related_ponds' should be available from the previous cell execution\n",
    "# If not, we rely on the logic that this cell is run after Cell 5.\n",
    "if 'related_ponds' not in locals():\n",
    "    print(\"Warning: 'related_ponds' not found. Please ensure the previous cell is executed.\")\n",
    "    unique_stock_ids = set()\n",
    "else:\n",
    "    unique_stock_ids = set(pond.get('fishStockId') for pond in related_ponds if pond.get('fishStockId'))\n",
    "\n",
    "print(f\"Processing {len(unique_stock_ids)} unique Stock IDs associated with the current map.\")\n",
    "\n",
    "for stock_id in unique_stock_ids:\n",
    "    # Find all releases for this stock\n",
    "    # Note: Scanning all values in stock_release_config might be inefficient for very large datasets,\n",
    "    # but acceptable for this precompute scope.\n",
    "    stock_releases = [item for item in stock_release_config.values() if item.get('stockId') == stock_id]\n",
    "    \n",
    "    for sr in stock_releases:\n",
    "        release_id = sr.get('releaseId')\n",
    "        fish_quality_id = sr.get('fishId') # referred as fishId in stock_release.json, but actually qualityId\n",
    "        fish_env_affinity_id = sr.get('fishEnvId') # referred as fishEnvId in stock_release.json\n",
    "        \n",
    "        # Lookup Release Info\n",
    "        release_info = fish_release_config.get(str(release_id))\n",
    "        if not release_info:\n",
    "            # print(f\"Warning: Release ID {release_id} not found in fish_release.json\")\n",
    "            continue\n",
    "            \n",
    "        # Lookup Fish Quality Info\n",
    "        fish_info = basic_fish_quality_config.get(str(fish_quality_id))\n",
    "        species_id = fish_info.get('species', -1) if fish_info else -1\n",
    "            \n",
    "        row = {\n",
    "            'stockId': stock_id,\n",
    "            'releaseId': release_id,\n",
    "            'qualityId': fish_quality_id,\n",
    "            'envAffinityId': fish_env_affinity_id, # Renamed from envId for clarity\n",
    "            'speciesId': species_id,\n",
    "            \n",
    "            # Release Limits\n",
    "            'weight_min': release_info.get('weightMin'),\n",
    "            'weight_max': release_info.get('weightMax'),\n",
    "            'len_min': release_info.get('lengthMin'),\n",
    "            'len_max': release_info.get('lengthMax'),\n",
    "\n",
    "            # Environment Coefficients (Added per request)\n",
    "            'minEnvCoeff': release_info.get('minEnvCoeff', 0),\n",
    "            'minAdaptCoeff': release_info.get('minAdaptCoeff', 0),\n",
    "            \n",
    "            # Debug/Display info\n",
    "            'name': release_info.get('name'),\n",
    "            'probWeight': release_info.get('probWeightIdeal')\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "stockFishesPd = pd.DataFrame(rows)\n",
    "print(f\"Created stockFishesPd with {len(stockFishesPd)} rows.\")\n",
    "if not stockFishesPd.empty:\n",
    "    print(stockFishesPd.head().to_string())\n",
    "    # print(\"\\nColumn Types:\")\n",
    "    # print(stockFishesPd.dtypes)\n",
    "else:\n",
    "    print(\"DataFrame is empty. Check if stock_release.json maps correctly to the pond stock IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e08507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续进行后续数据关联 (Data Enrichment Phase II)\n",
    "\n",
    "# 8. 环境亲和性关联 (Environment Affinity Lookup):\n",
    "#    - 目标: 丰富鱼类的环境适应参数。\n",
    "#    - 操作: 使用 `envAffinityId` (原 `fishEnvId`) 关联 `fish_env_affinity.json`。\n",
    "#    - 提取关键属性 (Attributes Extraction):\n",
    "#         - 基础ID关联: structId (结构), tempId (温度), layerId (水层), lightId (光照)。\n",
    "#         - 诱鱼系数: baitCoeffGroup, baitTypeCoeffGroup, periodCoeffGroup (时段)。\n",
    "#         - 适应性参数: \n",
    "#             - pressureSensitivity (气压敏感度)\n",
    "#             - minAdaptLureRatio / maxAdaptLureRatio (路亚适应比例)\n",
    "#             - maxAcceptLengthRatio (最大接受长度比)\n",
    "#         - 衰减配置: underLengthDecayCoeff / overLengthDecayCoeff (体型偏离衰减)。\n",
    "\n",
    "# 9. 结构体亲和性级联查找 (Structure Affinity Cascade):\n",
    "#    - 目标: 获取具体的物理结构交互参数。\n",
    "#    - 操作: 使用步骤 8 获得的 `structId`，查询 `struct_affinity.json`。\n",
    "#    - 提取参数 (Parameters Extraction): \n",
    "#         - `List`: 包含 `structType` (结构类型) 和 `coeff` (系数) 的列表。\n",
    "\n",
    "# 10. 温度亲和性级联查找 (Temperature Affinity Cascade):\n",
    "#    - 目标: 获取鱼类对温度的敏感度配置。\n",
    "#    - 操作: 使用步骤 8 获得的 `tempId`，查询 `temp_affinity.json`。\n",
    "#    - 提取参数 (Parameters Extraction): \n",
    "#         - `temperatureFav`: 最适温度 (注意可能需要缩放，如 220 -> 22.0)。\n",
    "#         - `tempAffectedRatio`: 温度影响比率。\n",
    "#         - `tempThreshold`: 温度容忍阈值。\n",
    "\n",
    "# 11. 水层亲和性级联查找 (Water Layer Affinity Cascade):\n",
    "#    - 目标: 获取鱼类在不同水层的分布偏好。\n",
    "#    - 操作: 使用步骤 8 获得的 `layerId`，查询 `water_layer_affinity.json`。\n",
    "#    - 提取参数 (Parameters Extraction): \n",
    "#         - `List`: 包含 `layerType` (水层类型, 如上/中/下) 和 `coeff` (系数) 的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "992238dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Affinity Configs...\n",
      "Affinity Configs loaded.\n",
      "Enriching DataFrame...\n",
      "Enrichment Complete.\n",
      "New DataFrame Shape: (35, 31)\n",
      "   qualityId  envAffinityId  temperatureFav  structId   tempId                                                                                   layerList                                                                                                                                                                                                                                                                                                                                                                                                       structList  periodCoeffGroup\n",
      "0  101034430        1013390             195   2011030  2021010  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]     [{'structType': 0, 'coeff': 1}, {'structType': 1, 'coeff': 0.3}, {'structType': 2, 'coeff': 0.3}, {'structType': 3, 'coeff': 0.3}, {'structType': 4, 'coeff': 0.3}, {'structType': 5, 'coeff': 0.3}, {'structType': 6, 'coeff': 0.3}, {'structType': 7, 'coeff': 0.3}, {'structType': 8, 'coeff': 0.3}, {'structType': 9, 'coeff': 0.3}, {'structType': 10, 'coeff': 0.3}, {'structType': 11, 'coeff': 0.3}]              2011\n",
      "1  101034090        1013050             195   2010940  2020920  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]                           [{'structType': 0, 'coeff': 0}, {'structType': 1, 'coeff': 1}, {'structType': 2, 'coeff': 0}, {'structType': 3, 'coeff': 0}, {'structType': 4, 'coeff': 0}, {'structType': 5, 'coeff': 0}, {'structType': 6, 'coeff': 0}, {'structType': 7, 'coeff': 0}, {'structType': 8, 'coeff': 0}, {'structType': 9, 'coeff': 0}, {'structType': 10, 'coeff': 0}, {'structType': 11, 'coeff': 0}]              2011\n",
      "2  101031007        1010066             195   2010940  2020920  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]                           [{'structType': 0, 'coeff': 0}, {'structType': 1, 'coeff': 1}, {'structType': 2, 'coeff': 0}, {'structType': 3, 'coeff': 0}, {'structType': 4, 'coeff': 0}, {'structType': 5, 'coeff': 0}, {'structType': 6, 'coeff': 0}, {'structType': 7, 'coeff': 0}, {'structType': 8, 'coeff': 0}, {'structType': 9, 'coeff': 0}, {'structType': 10, 'coeff': 0}, {'structType': 11, 'coeff': 0}]              2011\n",
      "3  101034450        1013410             195   2011040  2021020  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]                           [{'structType': 0, 'coeff': 0}, {'structType': 1, 'coeff': 1}, {'structType': 2, 'coeff': 0}, {'structType': 3, 'coeff': 0}, {'structType': 4, 'coeff': 0}, {'structType': 5, 'coeff': 0}, {'structType': 6, 'coeff': 0}, {'structType': 7, 'coeff': 0}, {'structType': 8, 'coeff': 0}, {'structType': 9, 'coeff': 0}, {'structType': 10, 'coeff': 0}, {'structType': 11, 'coeff': 0}]              2011\n",
      "4  101034510        1013470             220   2011060  2021040  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]  [{'structType': 0, 'coeff': 1}, {'structType': 1, 'coeff': 1}, {'structType': 2, 'coeff': 0.3}, {'structType': 3, 'coeff': 1}, {'structType': 4, 'coeff': 0.2}, {'structType': 5, 'coeff': 0.05}, {'structType': 6, 'coeff': 0.05}, {'structType': 7, 'coeff': 0.05}, {'structType': 8, 'coeff': 0.05}, {'structType': 9, 'coeff': 0.05}, {'structType': 10, 'coeff': 0.05}, {'structType': 11, 'coeff': 0.05}]              2011\n"
     ]
    }
   ],
   "source": [
    "# 9-11. 实现环境与亲和性级联查找 (Env & Affinity Cascade Lookup)\n",
    "\n",
    "print(\"Loading Affinity Configs...\")\n",
    "# 1. 加载所有亲和性配置 (Load Configs)\n",
    "with open(DATA_ROOT / 'fish_env_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    env_affinity_config = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'struct_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    struct_affinity_config = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'temp_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    temp_affinity_config = json.load(f)\n",
    "\n",
    "with open(DATA_ROOT / 'water_layer_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    layer_affinity_config = json.load(f)\n",
    "print(\"Affinity Configs loaded.\")\n",
    "\n",
    "# 2. 准备查找字典 (Prepare Lookup Dicts)\n",
    "#    优化: 直接构建 id -> data 的快速查找字典，避免每次遍历 list\n",
    "#    注意: JSON key通常是字符串, DataFrame中Id可能是int, 查找时需注意类型转换\n",
    "\n",
    "def get_config_by_id(config_dict, target_id):\n",
    "    \"\"\"Safe lookup helper handling str/int key mismatch\"\"\"\n",
    "    if target_id is None:\n",
    "        return None\n",
    "    return config_dict.get(str(target_id))\n",
    "\n",
    "# 3. 扩展 DataFrame (Enrich DataFrame)\n",
    "#    虽然可以使用 apply，但在列数较多且逻辑复杂时，迭代或列表推导式便于调试和错误处理\n",
    "#    考虑到数据量不大 (几十到几百行)，直接遍历 row 更新字典列表然后重新创建 DF 也是一种清晰的方法\n",
    "#    或者使用 apply + Series expand\n",
    "\n",
    "def enrich_row(row):\n",
    "    # Step 8: Env Affinity Lookup\n",
    "    env_id = row.get('envAffinityId')\n",
    "    env_info = get_config_by_id(env_affinity_config, env_id)\n",
    "    \n",
    "    extra_data = {}\n",
    "    \n",
    "    if env_info:\n",
    "        # Extract basic Env IDs\n",
    "        struct_id = env_info.get('structId')\n",
    "        temp_id = env_info.get('tempId')\n",
    "        layer_id = env_info.get('layerId')\n",
    "        light_id = env_info.get('lightId')\n",
    "        \n",
    "        extra_data.update({\n",
    "            'structId': struct_id,\n",
    "            'tempId': temp_id,\n",
    "            'layerId': layer_id,\n",
    "            'lightId': light_id,\n",
    "            # Coeffs\n",
    "            'baitCoeffGroup': env_info.get('baitCoeffGroup'),\n",
    "            'baitTypeCoeffGroup': env_info.get('baitTypeCoeffGroup'),\n",
    "            'periodCoeffGroup': env_info.get('periodCoeffGroup'),\n",
    "            # Adaptability Stats\n",
    "            'pressureSensitivity': env_info.get('pressureSensitivity'),\n",
    "            'minAdaptLureRatio': env_info.get('minAdaptLureRatio'),\n",
    "            'maxAdaptLureRatio': env_info.get('maxAdaptLureRatio'),\n",
    "            'maxAcceptLengthRatio': env_info.get('maxAcceptLengthRatio'),\n",
    "            'underLengthDecayCoeff': env_info.get('underLengthDecayCoeff'),\n",
    "            'overLengthDecayCoeff': env_info.get('overLengthDecayCoeff'),\n",
    "        })\n",
    "        \n",
    "        # Step 9: Structure Affinity Cascade\n",
    "        struct_info = get_config_by_id(struct_affinity_config, struct_id)\n",
    "        if struct_info:\n",
    "            extra_data['structList'] = struct_info.get('List') # raw list of {structType, coeff}\n",
    "        \n",
    "        # Step 10: Temperature Affinity Cascade\n",
    "        temp_info = get_config_by_id(temp_affinity_config, temp_id)\n",
    "        if temp_info:\n",
    "            extra_data['temperatureFav'] = temp_info.get('temperatureFav')\n",
    "            extra_data['tempAffectedRatio'] = temp_info.get('tempAffectedRatio')\n",
    "            extra_data['tempThreshold'] = temp_info.get('tempThreshold')\n",
    "            \n",
    "        # Step 11: Water Layer Affinity Cascade\n",
    "        layer_info = get_config_by_id(layer_affinity_config, layer_id)\n",
    "        if layer_info:\n",
    "            extra_data['layerList'] = layer_info.get('List') # raw list of {layerType, coeff}\n",
    "            \n",
    "    return pd.Series(extra_data)\n",
    "\n",
    "# 应用扩展逻辑\n",
    "print(\"Enriching DataFrame...\")\n",
    "if not stockFishesPd.empty:\n",
    "    enriched_columns = stockFishesPd.apply(enrich_row, axis=1)\n",
    "    \n",
    "    # Concatenate original df with new columns\n",
    "    stockFishesPd = pd.concat([stockFishesPd, enriched_columns], axis=1)\n",
    "    \n",
    "    print(\"Enrichment Complete.\")\n",
    "    print(f\"New DataFrame Shape: {stockFishesPd.shape}\")\n",
    "    print(stockFishesPd[['qualityId','envAffinityId','temperatureFav', 'structId', 'tempId', 'layerList', 'structList', 'periodCoeffGroup']].head().to_string())\n",
    "else:\n",
    "    print(\"stockFishesPd is empty, skipping enrichment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b54414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bait/Period Configs...\n",
      "Aggregating Groups...\n",
      "Enriching StockFishesPd with Bait/Period lists...\n",
      "Enrichment Complete.\n",
      "DataFrame Shape: (35, 34)\n",
      "Sample Row 0 - QualityID: 101034430\n",
      "BaitCoeffGroup: 80000\n",
      "BaitList Count: 1\n",
      "PeriodList Count: 8\n"
     ]
    }
   ],
   "source": [
    "# 12-14. 诱鱼与时段亲和性关联 (Bait & Period Affinity)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading Bait/Period Configs...\")\n",
    "# Load JSONs\n",
    "with open(DATA_ROOT / 'bait_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    bait_affinity_data = json.load(f)\n",
    "with open(DATA_ROOT / 'bait_type_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    bait_type_affinity_data = json.load(f)\n",
    "with open(DATA_ROOT / 'period_affinity.json', 'r', encoding='utf-8') as f:\n",
    "    period_affinity_data = json.load(f)\n",
    "\n",
    "# Helper to aggregate by group\n",
    "# Transform {id: {data}} -> {group_id: [data_list]}\n",
    "def aggregate_by_group(source_data, group_key_name):\n",
    "    grouped = {}\n",
    "    for item in source_data.values():\n",
    "        grp_id = item.get(group_key_name)\n",
    "        \n",
    "        # Safe cast to string for consistent lookup key\n",
    "        if grp_id is not None:\n",
    "            grp_key = str(int(grp_id)) # int -> str to match potential int IDs\n",
    "            if grp_key not in grouped:\n",
    "                grouped[grp_key] = []\n",
    "            grouped[grp_key].append(item)\n",
    "    return grouped\n",
    "\n",
    "print(\"Aggregating Groups...\")\n",
    "# Note: 'baitCoeffGroup' matches the field in fish_env_affinity\n",
    "bait_groups = aggregate_by_group(bait_affinity_data, 'baitCoeffGroup') \n",
    "bait_type_groups = aggregate_by_group(bait_type_affinity_data, 'baitTypeCoeffGroup') \n",
    "\n",
    "# Note: In period_affinity.json, the key is 'periodGroup', but in fish_env it is 'periodCoeffGroup'\n",
    "period_groups = aggregate_by_group(period_affinity_data, 'periodGroup')\n",
    "\n",
    "# Enrich Wrapper\n",
    "def enrich_bait_period(row):\n",
    "    # Lookup Bait Group\n",
    "    # row['baitCoeffGroup'] comes from fish_env_affinity, expected to be int or str\n",
    "    b_val = row.get('baitCoeffGroup')\n",
    "    if pd.notnull(b_val):\n",
    "        b_grp = str(int(b_val))\n",
    "        bait_list = bait_groups.get(b_grp, [])\n",
    "    else:\n",
    "        bait_list = []\n",
    "    \n",
    "    # Lookup Bait Type Group\n",
    "    bt_val = row.get('baitTypeCoeffGroup')\n",
    "    if pd.notnull(bt_val):\n",
    "        bt_grp = str(int(bt_val))\n",
    "        bait_type_list = bait_type_groups.get(bt_grp, [])\n",
    "    else:\n",
    "        bait_type_list = []\n",
    "    \n",
    "    # Lookup Period Group\n",
    "    p_val = row.get('periodCoeffGroup')\n",
    "    if pd.notnull(p_val):\n",
    "        p_grp = str(int(p_val))\n",
    "        period_list = period_groups.get(p_grp, [])\n",
    "    else:\n",
    "        period_list = []\n",
    "    \n",
    "    return pd.Series({\n",
    "        'baitList': bait_list,          # detailed list of {baitId, coeff...}\n",
    "        'baitTypeList': bait_type_list, # detailed list of {baitSubType, coeff...}\n",
    "        'periodList': period_list       # detailed list of {periodId, activityFactor...}\n",
    "    })\n",
    "\n",
    "print(\"Enriching StockFishesPd with Bait/Period lists...\")\n",
    "if not stockFishesPd.empty:\n",
    "    bp_columns = stockFishesPd.apply(enrich_bait_period, axis=1)\n",
    "    \n",
    "    # Concatenate\n",
    "    # Drop existing if re-running to avoid dupe columns\n",
    "    cols_to_drop = [c for c in ['baitList', 'baitTypeList', 'periodList'] if c in stockFishesPd.columns]\n",
    "    if cols_to_drop:\n",
    "         stockFishesPd = stockFishesPd.drop(columns=cols_to_drop)\n",
    "            \n",
    "    stockFishesPd = pd.concat([stockFishesPd, bp_columns], axis=1)\n",
    "    \n",
    "    print(\"Enrichment Complete.\")\n",
    "    print(f\"DataFrame Shape: {stockFishesPd.shape}\")\n",
    "    \n",
    "    # Sample Output\n",
    "    # Only show limited info\n",
    "    cols_check = ['qualityId', 'baitCoeffGroup', 'baitList', 'periodList']\n",
    "    # Just print the first row nicely formatted\n",
    "    first_row = stockFishesPd.iloc[0]\n",
    "    print(f\"Sample Row 0 - QualityID: {first_row['qualityId']}\")\n",
    "    print(f\"BaitCoeffGroup: {first_row['baitCoeffGroup']}\")\n",
    "    print(f\"BaitList Count: {len(first_row['baitList'])}\")\n",
    "    print(f\"PeriodList Count: {len(first_row['periodList'])}\")\n",
    "else:\n",
    "    print(\"DataFrame empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matrix_conversion_title",
   "metadata": {},
   "source": [
    "### 核心计算准备 1：亲和度配置数据构建 (Affinity DataFrame Construction)\n",
    "按照 Technical Guide 要求，将配置数据构建为**可查找的 DataFrame** (行=Fish, 列=Feature)，并在进入计算与广播阶段前转换为稠密矩阵。\n",
    "\n",
    "*   **StructAffinityDataFrame**: `Row: FishID, Col: StructTypeID` (用于 Gather 查找)\n",
    "*   **LayerAffinityDataFrame**: `Row: FishID, Col: LayerTypeID`\n",
    "*   **TempAffinityDataFrame**: `Row: FishID, Col: [Fav, Ratio, Threshold]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "matrix_conversion_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to Dense Matrices (via DataFrames)...\n",
      "Building Temp Affinity DataFrame...\n",
      "Temp DF Head (Indexed by QualityId):\n",
      "           temperatureFav  tempAffectedRatio  tempThreshold\n",
      "qualityId                                                  \n",
      "101034430            19.5            0.00105            0.0\n",
      "101034090            19.5            0.00105            0.0\n",
      "101031007            19.5            0.00105            0.0\n",
      "101034450            19.5            0.00105            0.0\n",
      "101034510            22.0            0.00100            0.0\n",
      "\n",
      "Building Struct Affinity DataFrame...\n",
      "Struct DF Head (Cols 0-5, Indexed by QualityId, Named Columns):\n",
      "           struct_0  struct_1  struct_2  struct_3  struct_4  struct_5\n",
      "101034430       1.0       0.3       0.3       0.3       0.3      0.30\n",
      "101034090       0.0       1.0       0.0       0.0       0.0      0.00\n",
      "101031007       0.0       1.0       0.0       0.0       0.0      0.00\n",
      "101034450       0.0       1.0       0.0       0.0       0.0      0.00\n",
      "101034510       1.0       1.0       0.3       1.0       0.2      0.05\n",
      "\n",
      "Building Layer Affinity DataFrame...\n",
      "Layer DF Head (Indexed by QualityId, Named Columns):\n",
      "           layer_1  layer_2  layer_3\n",
      "101034430        1        1      1.0\n",
      "101034090        1        1      1.0\n",
      "101031007        1        1      1.0\n",
      "101034450        1        1      1.0\n",
      "101034510        1        1      1.0\n",
      "\n",
      "Final Struct Matrix: (35, 12) (Cols: 0..11)\n",
      "Final Layer Matrix:  (35, 3) (Cols: 1..3)\n",
      "Final Temp Params:   (35, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 定义常量\n",
    "# Struct Constants (0-11 based on data_formula.md)\n",
    "VALID_STRUCT_TYPES = list(range(12)) # 0..11\n",
    "# Layer Constants (1-3 based on data_formula.md)\n",
    "VALID_LAYER_TYPES = [1, 2, 3]\n",
    "\n",
    "def build_dense_matrices(df):\n",
    "    num_fishes = len(df)\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # Quality ID Indexing\n",
    "    # ----------------------------------------------------\n",
    "    if 'qualityId' not in df.columns:\n",
    "         print(\"Error: 'qualityId' not found in DataFrame columns!\")\n",
    "         raise KeyError(\"qualityId missing\")\n",
    "         \n",
    "    fish_quality_ids = df['qualityId'].values\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    # 1. Temp Affinity DataFrame\n",
    "    # ----------------------------------------------------\n",
    "    print(\"Building Temp Affinity DataFrame...\")\n",
    "    temp_cols = ['qualityId', 'temperatureFav', 'tempAffectedRatio', 'tempThreshold']\n",
    "    temp_df = df[temp_cols].copy()\n",
    "    temp_df.set_index('qualityId', inplace=True)\n",
    "    \n",
    "    # Normalize/Scale Values\n",
    "    temp_df['temperatureFav'] = temp_df['temperatureFav'] / 10.0\n",
    "    temp_df['tempAffectedRatio'] = temp_df['tempAffectedRatio'] / 10000.0\n",
    "    temp_df['tempThreshold'] = temp_df['tempThreshold'] / 10000.0\n",
    "    \n",
    "    if temp_df.isnull().values.any():\n",
    "        temp_df.fillna(0, inplace=True)\n",
    "\n",
    "    print(\"Temp DF Head (Indexed by QualityId):\")\n",
    "    print(temp_df.head())\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 2. Struct Affinity DataFrame (Named Columns: struct_0, struct_1...)\n",
    "    # ----------------------------------------------------\n",
    "    print(\"\\nBuilding Struct Affinity DataFrame...\")\n",
    "    struct_rows = []\n",
    "    struct_col_map = {t: f'struct_{t}' for t in VALID_STRUCT_TYPES}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        s_list = row.get('structList')\n",
    "        row_dict = {}\n",
    "        if isinstance(s_list, list):\n",
    "            for item in s_list:\n",
    "                s_type = item.get('structType')\n",
    "                coeff = item.get('coeff')\n",
    "                if s_type in struct_col_map:\n",
    "                    row_dict[struct_col_map[s_type]] = coeff\n",
    "        struct_rows.append(row_dict)\n",
    "    \n",
    "    struct_df = pd.DataFrame(struct_rows, index=fish_quality_ids)\n",
    "    \n",
    "    # Ensure all valid struct columns exist\n",
    "    struct_target_cols = [struct_col_map[t] for t in VALID_STRUCT_TYPES]\n",
    "    struct_df = struct_df.reindex(columns=struct_target_cols, fill_value=0.0) \n",
    "    struct_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    print(\"Struct DF Head (Cols 0-5, Indexed by QualityId, Named Columns):\")\n",
    "    print(struct_df.iloc[:, :6].head())\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 3. Layer Affinity DataFrame (Named Columns: layer_1, layer_2...)\n",
    "    # ----------------------------------------------------\n",
    "    print(\"\\nBuilding Layer Affinity DataFrame...\")\n",
    "    layer_rows = []\n",
    "    layer_col_map = {t: f'layer_{t}' for t in VALID_LAYER_TYPES}\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        l_list = row.get('layerList')\n",
    "        row_dict = {}\n",
    "        if isinstance(l_list, list):\n",
    "            for item in l_list:\n",
    "                l_type = item.get('layerType')\n",
    "                coeff = item.get('coeff')\n",
    "                if l_type in layer_col_map:\n",
    "                    row_dict[layer_col_map[l_type]] = coeff\n",
    "        layer_rows.append(row_dict)\n",
    "        \n",
    "    layer_df = pd.DataFrame(layer_rows, index=fish_quality_ids)\n",
    "    \n",
    "    # Ensure all valid layer columns exist\n",
    "    layer_target_cols = [layer_col_map[t] for t in VALID_LAYER_TYPES]\n",
    "    layer_df = layer_df.reindex(columns=layer_target_cols, fill_value=0.0)\n",
    "    layer_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    print(\"Layer DF Head (Indexed by QualityId, Named Columns):\")\n",
    "    print(layer_df.head())\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # 4. Convert to Numpy (float16)\n",
    "    # ----------------------------------------------------\n",
    "    # Struct: Ensure strict order 0..11\n",
    "    m_struct = struct_df[struct_target_cols].values.astype(np.float16)\n",
    "    \n",
    "    # Layer: Ensure strict order 1..3\n",
    "    m_layer = layer_df[layer_target_cols].values.astype(np.float16)\n",
    "    \n",
    "    m_temp = temp_df.values.astype(np.float16)\n",
    "    \n",
    "    return m_struct, m_layer, m_temp, temp_df, struct_df, layer_df\n",
    "\n",
    "print(\"Converting to Dense Matrices (via DataFrames)...\")\n",
    "if 'stockFishesPd' in locals():\n",
    "    m_struct, m_layer, m_temp, _, _, _ = build_dense_matrices(stockFishesPd)\n",
    "\n",
    "    print(f\"\\nFinal Struct Matrix: {m_struct.shape} (Cols: 0..11)\")\n",
    "    print(f\"Final Layer Matrix:  {m_layer.shape} (Cols: 1..3)\")\n",
    "    print(f\"Final Temp Params:   {m_temp.shape}\")\n",
    "else:\n",
    "    print(\"Error: stockFishesPd is not defined. Run setup cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec31b1",
   "metadata": {},
   "source": [
    "### 核心计算准备 1.5：时段亲和度配置构建 (Period Affinity DataFrame Construction)\n",
    "\n",
    "构建 `df_period_affinity`，关联 Fish Release -> End -> Period Group -> Period Activity.\n",
    "目标是得到每个 Fish Release 在每个时段 (PeriodId) 的活跃度系数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba0edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish Release Count: 419\n",
      "Wide Period DF Shape: (419, 8)\n",
      "\n",
      "Wide Period DataFrame Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodName</th>\n",
       "      <th>period0_3</th>\n",
       "      <th>period12_15</th>\n",
       "      <th>period15_18</th>\n",
       "      <th>period18_21</th>\n",
       "      <th>period21_24</th>\n",
       "      <th>period3_6</th>\n",
       "      <th>period6_9</th>\n",
       "      <th>period9_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>releaseId</th>\n",
       "      <th>fishName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300010</th>\n",
       "      <th>Release_Alewife_Young_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300020</th>\n",
       "      <th>Release_Alewife_Common_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300030</th>\n",
       "      <th>Release_Alewife_Trophy_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300040</th>\n",
       "      <th>Release_Alewife_Unique_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300050</th>\n",
       "      <th>Release_Alewife_Apex_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "periodName                            period0_3  period12_15  period15_18  \\\n",
       "releaseId fishName                                                          \n",
       "300010    Release_Alewife_Young_dew         1.0          1.0          1.0   \n",
       "300020    Release_Alewife_Common_dew        1.0          1.0          1.0   \n",
       "300030    Release_Alewife_Trophy_dew        1.0          1.0          1.0   \n",
       "300040    Release_Alewife_Unique_dew        1.0          1.0          1.0   \n",
       "300050    Release_Alewife_Apex_dew          1.0          1.0          1.0   \n",
       "\n",
       "periodName                            period18_21  period21_24  period3_6  \\\n",
       "releaseId fishName                                                          \n",
       "300010    Release_Alewife_Young_dew           1.0          1.0        1.0   \n",
       "300020    Release_Alewife_Common_dew          1.0          1.0        1.0   \n",
       "300030    Release_Alewife_Trophy_dew          1.0          1.0        1.0   \n",
       "300040    Release_Alewife_Unique_dew          1.0          1.0        1.0   \n",
       "300050    Release_Alewife_Apex_dew            1.0          1.0        1.0   \n",
       "\n",
       "periodName                            period6_9  period9_12  \n",
       "releaseId fishName                                           \n",
       "300010    Release_Alewife_Young_dew         1.0         1.0  \n",
       "300020    Release_Alewife_Common_dew        1.0         1.0  \n",
       "300030    Release_Alewife_Trophy_dew        1.0         1.0  \n",
       "300040    Release_Alewife_Unique_dew        1.0         1.0  \n",
       "300050    Release_Alewife_Apex_dew          1.0         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data for 'Release_Alewife_Young_dew':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periodName</th>\n",
       "      <th>period0_3</th>\n",
       "      <th>period12_15</th>\n",
       "      <th>period15_18</th>\n",
       "      <th>period18_21</th>\n",
       "      <th>period21_24</th>\n",
       "      <th>period3_6</th>\n",
       "      <th>period6_9</th>\n",
       "      <th>period9_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>releaseId</th>\n",
       "      <th>fishName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300010</th>\n",
       "      <th>Release_Alewife_Young_dew</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "periodName                           period0_3  period12_15  period15_18  \\\n",
       "releaseId fishName                                                         \n",
       "300010    Release_Alewife_Young_dew        1.0          1.0          1.0   \n",
       "\n",
       "periodName                           period18_21  period21_24  period3_6  \\\n",
       "releaseId fishName                                                         \n",
       "300010    Release_Alewife_Young_dew          1.0          1.0        1.0   \n",
       "\n",
       "periodName                           period6_9  period9_12  \n",
       "releaseId fishName                                          \n",
       "300010    Release_Alewife_Young_dew        1.0         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load Source JSONs\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "data_root = r\"d:\\fishinggame\\precompute\\data\\1\\1001\"\n",
    "\n",
    "with open(f\"{data_root}\\\\fish_release.json\", 'r', encoding='utf-8') as f:\n",
    "    fish_release_data = json.load(f)\n",
    "\n",
    "with open(f\"{data_root}\\\\stock_release.json\", 'r', encoding='utf-8') as f:\n",
    "    stock_release_data = json.load(f)\n",
    "\n",
    "with open(f\"{data_root}\\\\fish_env_affinity.json\", 'r', encoding='utf-8') as f:\n",
    "    fish_env_data = json.load(f)\n",
    "\n",
    "with open(f\"{data_root}\\\\period_affinity.json\", 'r', encoding='utf-8') as f:\n",
    "    period_affinity_data = json.load(f)\n",
    "\n",
    "# 2. Convert to DataFrames\n",
    "df_fish_release = pd.DataFrame.from_dict(fish_release_data, orient='index')\n",
    "df_stock_release = pd.DataFrame.from_dict(stock_release_data, orient='index')\n",
    "df_fish_env = pd.DataFrame.from_dict(fish_env_data, orient='index')\n",
    "df_period_affinity_list = pd.DataFrame.from_dict(period_affinity_data, orient='index')\n",
    "\n",
    "# 3. Rename columns to avoid collisions and clarify semantics\n",
    "df_fish_release = df_fish_release.rename(columns={'id': 'releaseId', 'name': 'fishName'})\n",
    "df_stock_release = df_stock_release.rename(columns={'id': 'mappingId'}) # stock_release id is just a mapping id\n",
    "df_fish_env = df_fish_env.rename(columns={'id': 'envId', 'name': 'envName', 'periodCoeffGroup': 'periodGroupLink'})\n",
    "df_period_affinity_list = df_period_affinity_list.rename(columns={'id': 'periodEntryId'})\n",
    "\n",
    "# 4. Perform Merges (Long Format Construction)\n",
    "\n",
    "# Step 4a: Fish Release -> Stock Release (Link via releaseId)\n",
    "df_merged_1 = pd.merge(\n",
    "    df_fish_release[['releaseId', 'fishName']], \n",
    "    df_stock_release[['releaseId', 'fishEnvId']], \n",
    "    on='releaseId', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 4b: -> Fish Env (Link via fishEnvId == envId)\n",
    "df_merged_2 = pd.merge(\n",
    "    df_merged_1,\n",
    "    df_fish_env[['envId', 'periodGroupLink']],\n",
    "    left_on='fishEnvId',\n",
    "    right_on='envId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 4c: -> Period Affinity (Link via periodGroupLink == periodGroup)\n",
    "df_period_long = pd.merge(\n",
    "    df_merged_2,\n",
    "    df_period_affinity_list[['periodGroup', 'periodId', 'periodActivityFactor']],\n",
    "    left_on='periodGroupLink',\n",
    "    right_on='periodGroup',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Pivot to Wide Format (User Request: Named Columns)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Define Mapping (Based on data_formula.md)\n",
    "period_id_to_name = {\n",
    "    101060001: \"period6_9\",\n",
    "    101060002: \"period9_12\",\n",
    "    101060003: \"period12_15\",\n",
    "    101060004: \"period15_18\",\n",
    "    101060005: \"period18_21\",\n",
    "    101060006: \"period21_24\",\n",
    "    101060007: \"period0_3\",\n",
    "    101060008: \"period3_6\"\n",
    "}\n",
    "\n",
    "# Create 'periodName' column\n",
    "df_period_long['periodName'] = df_period_long['periodId'].map(period_id_to_name)\n",
    "\n",
    "# Pivot: Index=[releaseId, fishName], Columns=periodName, Values=periodActivityFactor\n",
    "df_period_wide = df_period_long.pivot_table(\n",
    "    index=['releaseId', 'fishName'], \n",
    "    columns='periodName', \n",
    "    values='periodActivityFactor',\n",
    "    fill_value=0 # Default to 0 if missing\n",
    ")\n",
    "\n",
    "# Reset index to make releaseId a column again (optional, depending on usage)\n",
    "# df_period_wide.reset_index(inplace=True)\n",
    "\n",
    "print(f\"Fish Release Count: {len(df_fish_release)}\")\n",
    "print(f\"Wide Period DF Shape: {df_period_wide.shape}\")\n",
    "print(\"\\nWide Period DataFrame Head:\")\n",
    "display(df_period_wide.head())\n",
    "\n",
    "# Check for a specific fish\n",
    "sample_name = \"Release_Alewife_Young_dew\"\n",
    "# Filter using index level if checking by name\n",
    "if sample_name in df_period_wide.index.get_level_values('fishName'):\n",
    "    print(f\"\\nSample Data for '{sample_name}':\")\n",
    "    display(df_period_wide.query(f\"fishName == '{sample_name}'\"))\n",
    "else:\n",
    "    print(f\"\\nSample fish '{sample_name}' not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voxel_loading_title",
   "metadata": {},
   "source": [
    "### 核心计算准备 2：加载体素数据 (Load Voxel Data)\n",
    "读取 `Global.npy`，提取 Structural Slots 和 Depth Layer 信息。\n",
    "\n",
    "*   **Data**: `[X, Z, C]`\n",
    "*   **Target**: `struct_slots [X, Z, 3]`, `depth_layer [X, Z]` (+ Y extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "voxel_loading_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Voxel Data from: D:\\fishinggame\\ExportedData\\Fishing_1006001_Dense_20260107_154037\\Fishing_1006001_Global.npy\n",
      "Loaded Voxel Data Shape: (134, 8, 134), dtype=int32\n",
      "Struct Bitmask Map Shape: (134, 8, 134)\n",
      "Sample Bitmask Values: [ 0  1  2  3  4  5  8  9 12 13]\n"
     ]
    }
   ],
   "source": [
    "# Load Voxel Data\n",
    "# File Path: .../Fishing_1006001_Global.npy\n",
    "# Based on map_data.json, this is a Dense 3D array [X, Y, Z].\n",
    "print(f\"Loading Voxel Data from: {test_path}\")\n",
    "voxel_data = np.load(test_path)\n",
    "\n",
    "print(f\"Loaded Voxel Data Shape: {voxel_data.shape}, dtype={voxel_data.dtype}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Voxel Data Interpretation (Bitmask)\n",
    "# ------------------------------------------------------\n",
    "# According to VoxelMapDataFormat.md, the int32 value is a Bitmask.\n",
    "# Bit 0: Water, Bit 1: Grass, Bit 2: Stone, ...\n",
    "# StructType IDs in 'struct_affinity.json' (0, 1, 2...) correspond to these Bit Indices.\n",
    "\n",
    "# We will use the raw 3D bitmask array for calculation.\n",
    "# struct_slots_map is essentially the voxel_data itself.\n",
    "struct_bitmask_map = voxel_data.astype(np.int32)\n",
    "\n",
    "# Depth/Layer info is DERIVED from Y coordinate or separate channel?\n",
    "# Note: The Global.npy appears to only contain the bitmask (int32).\n",
    "# If we need Depth Layer, we might need to compute it from the 'Water' bit distribution \n",
    "# or read a separate file if available. \n",
    "# For this DEMO, we will assume Layer 0 (Surface) to Layer N (Bottom) maps to Y indices.\n",
    "# Or we define a simple dummy depth map based on (Y / MaxY).\n",
    "dim_y = struct_bitmask_map.shape[1]\n",
    "# Create normalized depth [0, 1] for temp calculation. \n",
    "# 0 = Bottom, 1 = Surface? Or inverse. Usually Surface is Top (High Y).\n",
    "y_indices = np.arange(dim_y)\n",
    "normalized_depth_map = col_vec = y_indices[None, :, None] / float(dim_y) # [1, Y, 1] broadcastable\n",
    "\n",
    "print(f\"Struct Bitmask Map Shape: {struct_bitmask_map.shape}\")\n",
    "print(\"Sample Bitmask Values:\", np.unique(struct_bitmask_map.flatten())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core_calc_title",
   "metadata": {},
   "source": [
    "### 核心计算 3：批量亲和度计算 (Data-Driven Batch Calculation)\n",
    "\n",
    "1.  **AffStruct**: `StructMatrix` vs `StructSlotsMap` (Gather)\n",
    "2.  **AffTemp**: `TempParams` vs `DepthMap` (Gaussian)\n",
    "3.  **Synthesis**: `EnvCoeff`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for Constants in fish_env_data...\n",
      "Using Params: SurfaceT=25.0, BottomT=10.0, ToleranceWidth=50.0\n",
      "Base Weight Vector Shape: (35,)\n",
      "Re-calculating Struct Affinity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\futou\\AppData\\Local\\Temp\\ipykernel_3448\\3722323830.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  stockFishesPd['probWeightIdeal'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-calculating Temp Affinity (Parameterized & Fixed)...\n",
      "Sample t_ratio (Fixed): [1.05 1.05 1.05 1.05 1.  ]\n",
      "Aff Temp Mean: 0.7017224427795581\n",
      "Calculating Final Synthesis (BaseWeight * Struct * Temp)...\n",
      "Final Weight Tensor Shape: (134, 8, 134, 35)\n",
      "Max Weight: 249929.14836380148\n",
      "Mean Weight: 995.7949859033392\n",
      "Voxels with Weight > 0.01: 199787 / 5027680\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# RELOAD CONFIGS FOR PARAMETERIZED CALCULATION (Corrected Scaling)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# 1. Load Constants\n",
    "print(\"Searching for Constants in fish_env_data...\")\n",
    "const_key = None\n",
    "# (Same logic as before)\n",
    "TEMP_TOLERANCE_WIDTH = 6.0 \n",
    "\n",
    "# 2. Load Pond/Map Params (Mocked for Proof)\n",
    "POND_ID = 1001\n",
    "BOTTOM_T = 10.0 \n",
    "SURFACE_T = 25.0 \n",
    "\n",
    "print(f\"Using Params: SurfaceT={SURFACE_T}, BottomT={BOTTOM_T}, ToleranceWidth={TEMP_TOLERANCE_WIDTH}\")\n",
    "\n",
    "# 3. Base Weight\n",
    "if 'probWeightIdeal' not in stockFishesPd.columns:\n",
    "    stockFishesPd = pd.merge(\n",
    "        stockFishesPd, \n",
    "        df_fish_release[['releaseId', 'probWeightIdeal']], \n",
    "        left_on='releaseId', \n",
    "        right_on='releaseId', \n",
    "        how='left'\n",
    "    )\n",
    "    stockFishesPd['probWeightIdeal'].fillna(0, inplace=True)\n",
    "\n",
    "base_weight_vec = stockFishesPd['probWeightIdeal'].values.astype(np.float32)\n",
    "print(f\"Base Weight Vector Shape: {base_weight_vec.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# RE-ROUTINE CORE CALCULATION\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# A. Struct Affinity (Reuse)\n",
    "print(\"Re-calculating Struct Affinity...\")\n",
    "X, Y, Z_dim = struct_bitmask_map.shape\n",
    "F = m_struct.shape[0]\n",
    "S = m_struct.shape[1] \n",
    "aff_struct_final = np.zeros((X, Y, Z_dim, F), dtype=np.float16)\n",
    "RELEVANT_BITS = 12 \n",
    "for bit_idx in range(RELEVANT_BITS):\n",
    "    mask = (struct_bitmask_map >> bit_idx) & 1\n",
    "    if np.sum(mask) == 0: continue\n",
    "    if bit_idx < S:\n",
    "        coeffs = m_struct[:, bit_idx]\n",
    "        updates = mask[..., np.newaxis] * coeffs[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "        aff_struct_final = np.maximum(aff_struct_final, updates)\n",
    "\n",
    "# B. Temp Affinity (Parameterized & Fixed Scaling)\n",
    "print(\"Re-calculating Temp Affinity (Parameterized & Fixed)...\")\n",
    "t_map = SURFACE_T + (BOTTOM_T - SURFACE_T) * normalized_depth_map\n",
    "t_map_3d = np.tile(t_map, (X, 1, Z_dim)) \n",
    "t_map_4d = t_map_3d[..., np.newaxis]\n",
    "\n",
    "t_fav = m_temp[:, 0]\n",
    "# *** SCALING FIX ***\n",
    "# Cell 12 applied /10000. Raw is ~10. We want ~1.0. \n",
    "# So correct by *1000 (Resulting in Raw / 10).\n",
    "t_ratio = m_temp[:, 1] * 1000.0  \n",
    "\n",
    "print(f\"Sample t_ratio (Fixed): {t_ratio[:5]}\")\n",
    "\n",
    "# Formula: exp( - (T - Tfav)^2 / (Width * Ratio^2) )\n",
    "diff_sq = (t_map_4d - t_fav) ** 2\n",
    "denom = TEMP_TOLERANCE_WIDTH * (t_ratio ** 2)\n",
    "denom[denom < 1e-5] = 1e-5\n",
    "\n",
    "aff_temp_final = np.exp(- diff_sq / denom)\n",
    "print(f\"Aff Temp Mean: {np.mean(aff_temp_final)}\")\n",
    "\n",
    "# C. Synthesis\n",
    "print(\"Calculating Final Synthesis (BaseWeight * Struct * Temp)...\")\n",
    "\n",
    "# EnvCoeff = Struct * Temp (Using implicit max(0) since positive)\n",
    "env_coeff = aff_struct_final * aff_temp_final\n",
    "\n",
    "# Final Weight = BaseWeight * EnvCoeff\n",
    "final_weight_tensor = base_weight_vec[np.newaxis, np.newaxis, np.newaxis, :] * env_coeff\n",
    "\n",
    "print(f\"Final Weight Tensor Shape: {final_weight_tensor.shape}\")\n",
    "print(f\"Max Weight: {np.max(final_weight_tensor)}\")\n",
    "print(f\"Mean Weight: {np.mean(final_weight_tensor)}\")\n",
    "\n",
    "# Quick Histogram (Print counts)\n",
    "non_zero_count = np.sum(final_weight_tensor > 0.01)\n",
    "print(f\"Voxels with Weight > 0.01: {non_zero_count} / {final_weight_tensor.size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "core_calc_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Struct Affinity for Grid [134x8x134] and 35 Fish...\n",
      "AffStruct Final Shape: (134, 8, 134, 35)\n",
      "AffTemp Final Shape: (134, 8, 134, 35)\n",
      "EnvCoeff Final Shape: (134, 8, 134, 35)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# A. Struct Affinity Calculation (Bit-wise Iteration)\n",
    "# -----------------------------------------------------\n",
    "# Goal: affStruct[x, y, z, f] = Max over all PRESENT struct types\n",
    "# Input:\n",
    "#   struct_bitmask_map: [X, Y, Z] (int32)\n",
    "#   m_struct (DataFrame Matrix): [F, S] (Rows: Fish, Cols: StructTypeID)\n",
    "\n",
    "X, Y, Z_dim = struct_bitmask_map.shape\n",
    "F = m_struct.shape[0]\n",
    "S = m_struct.shape[1] # Max Struct Types (e.g., 30)\n",
    "\n",
    "print(f\"Calculating Struct Affinity for Grid [{X}x{Y}x{Z_dim}] and {F} Fish...\")\n",
    "\n",
    "# Initialize Result with zeros (or min affinity)\n",
    "aff_struct_final = np.zeros((X, Y, Z_dim, F), dtype=np.float16)\n",
    "\n",
    "# BITMASK_TO_STRUCT_ID Mapping\n",
    "# Assuming Bit K corresponds to StructTypeID K for now.\n",
    "# Iterate only relevant bits (e.g. 0 to 11)\n",
    "RELEVANT_BITS = 12 \n",
    "\n",
    "for bit_idx in range(RELEVANT_BITS):\n",
    "    # 1. Create Boolean Mask for this Struct Type\n",
    "    # Check if Bit is set: (Val >> bit) & 1\n",
    "    mask = (struct_bitmask_map >> bit_idx) & 1\n",
    "    # mask shape: [X, Y, Z], 0 or 1\n",
    "    \n",
    "    if np.sum(mask) == 0:\n",
    "        continue # Skip if this feature never appears\n",
    "        \n",
    "    # 2. Get Affinity Coeffs for this Struct Type (for all fish)\n",
    "    # m_struct[:, bit_idx] -> Shape [F]\n",
    "    if bit_idx < S:\n",
    "        coeffs = m_struct[:, bit_idx] # [F]\n",
    "        \n",
    "        # 3. Broadcast and Update Max\n",
    "        # mask: [X, Y, Z, 1]\n",
    "        # coeffs: [1, 1, 1, F]\n",
    "        # update: [X, Y, Z, F]\n",
    "        \n",
    "        # Optim: multiplication by boolean mask\n",
    "        # We want: where(mask, coeff, current_max)\n",
    "        # But simple max(current, mask * coeff) works if coeff >= 0\n",
    "        \n",
    "        updates = mask[..., np.newaxis] * coeffs[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "        aff_struct_final = np.maximum(aff_struct_final, updates)\n",
    "\n",
    "print(f\"AffStruct Final Shape: {aff_struct_final.shape}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# B. Temp Affinity Calculation (Using 3D/Y-based Depth)\n",
    "# -----------------------------------------------------\n",
    "# T_pixel = SurfaceT + (BottomT - SurfaceT) * Depth (0-1)\n",
    "SURFACE_T = 25.0\n",
    "BOTTOM_T = 10.0\n",
    "\n",
    "# BroadCast Pre-calc: normalized_depth_map is [1, Y, 1]\n",
    "# broadcast to [X, Y, Z]\n",
    "t_map = SURFACE_T + (BOTTOM_T - SURFACE_T) * normalized_depth_map\n",
    "t_map = np.tile(t_map, (X, 1, Z_dim)) # Expand to full grid if needed, or keep broadcast\n",
    "\n",
    "t_map_expanded = t_map[..., np.newaxis] # [X, Y, Z, 1]\n",
    "\n",
    "t_fav = m_temp[:, 0]\n",
    "t_ratio = m_temp[:, 1]\n",
    "WIDTH_CONST = 50.0 \n",
    "\n",
    "diff_sq = (t_map_expanded - t_fav) ** 2\n",
    "denom = WIDTH_CONST * (t_ratio ** 2)\n",
    "denom[denom < 1e-5] = 1e-5\n",
    "\n",
    "aff_temp_final = np.exp(- diff_sq / denom)\n",
    "print(f\"AffTemp Final Shape: {aff_temp_final.shape}\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# C. Synthesis\n",
    "# -----------------------------------------------------\n",
    "# EnvCoeff [X, Y, Z, F]\n",
    "env_coeff_final = aff_struct_final * aff_temp_final\n",
    "print(f\"EnvCoeff Final Shape: {env_coeff_final.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
