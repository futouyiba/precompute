{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "129c534e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import json\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a6baea6",
            "metadata": {},
            "source": [
                "### 链条式找配置，并组装紧密数组（每张地图只需做一次，可用于生成一串派生环境场）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "load_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "\"\"\"\n",
                "数据处理步骤 (Metaphysical Data Processing Steps):\n",
                "\n",
                "1.  **数据源与目标确立 (Source & Target Definition)**\n",
                "    - Source: 位于 `precompute/data` 下的静态配置表 (JSON)，定义了游戏世界的逻辑结构。\n",
                "    - Target: 位于 `ExportedData` 下的 Numpy 数组，代表了具体场景的物理数据 (Voxel/Grid)。\n",
                "\n",
                "2.  **标识符提取 (Identifier Extraction)**\n",
                "    - 从物理数据的文件名/路径中提取关键标识符 (SceneID / AssetID)。\n",
                "    - 例如: 从 `Fishing_1006001_Global.npy` 提取 `1006001`。\n",
                "\n",
                "3.  **逻辑映射建立 (Logical Mapping)**\n",
                "    - 利用中间件/配置表 (`map_scene.json`) 将物理标识符 (AssetID) 映射回系统逻辑标识符 (MapID)。\n",
                "    - 这一步是连接“即时演算数据”与“策划配置数据”的关键桥梁。\n",
                "\n",
                "4.  **上下文关联与整合 (Context Association & Integration)**\n",
                "    - 以 MapID 为锚点，级联查询关联的 Pond, Stock, Fish 配置。\n",
                "    - 将分散的数据整合为适合并行计算的结构化格式 (Numpy/Pandas)。\n",
                "\"\"\"\n",
                "\n",
                "# 配置路径\n",
                "DATA_ROOT = Path(r'D:\\fishinggame\\precompute\\data\\1\\1001')\n",
                "EXPORTED_DATA_ROOT = Path(r'D:\\fishinggame\\ExportedData')\n",
                "\n",
                "# 加载 map_scene.json\n",
                "with open(DATA_ROOT / 'map_scene.json', 'r', encoding='utf-8') as f:\n",
                "    map_scene = json.load(f)\n",
                "\n",
                "# 建立 assetId -> map_id 的反向索引\n",
                "asset_to_map = {info['assetId']: int(map_id) for map_id, info in map_scene.items() if info.get('assetId')}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "scene_id_lookup",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "文件路径: D:\\fishinggame\\ExportedData\\Fishing_1006001_Dense_20260107_154037\\Fishing_1006001_Global.npy\n",
                        "提取的 scene_id: 1006001\n",
                        "对应的 map_id: 1009\n",
                        "地图信息: {'id': 1009, 'name': 'map_base_6', 'desc': 106, 'assetId': '1006001', 'originOffsetX': 0, 'originOffsetY': 0, 'offsetX': 0, 'offsetY': 0, 'sizeX': 1000, 'sizeY': 1000, 'rotate': 0, 'mark': ''}\n"
                    ]
                }
            ],
            "source": [
                "def get_scene_id_from_path(npy_path: str) -> str:\n",
                "    \"\"\"从npy文件路径中提取scene_id (如 Fishing_1006001_Global.npy -> '1006001')\"\"\"\n",
                "    match = re.search(r'Fishing_(\\d+)', str(npy_path))\n",
                "    if not match:\n",
                "        raise ValueError(f'无法从路径中提取scene_id: {npy_path}')\n",
                "    return match.group(1)\n",
                "\n",
                "def get_map_id_from_scene_id(scene_id: str) -> int:\n",
                "    \"\"\"根据scene_id查找对应的map_id\"\"\"\n",
                "    if scene_id in asset_to_map:\n",
                "        return asset_to_map[scene_id]\n",
                "    raise ValueError(f'找不到scene_id {scene_id} 对应的map_id')\n",
                "\n",
                "def get_map_id_from_npy_path(npy_path: str) -> int:\n",
                "    \"\"\"从npy文件路径直接获取map_id\"\"\"\n",
                "    scene_id = get_scene_id_from_path(npy_path)\n",
                "    return get_map_id_from_scene_id(scene_id)\n",
                "\n",
                "# 测试示例\n",
                "test_path = r'D:\\fishinggame\\ExportedData\\Fishing_1006001_Dense_20260107_154037\\Fishing_1006001_Global.npy'\n",
                "scene_id = get_scene_id_from_path(test_path)\n",
                "map_id = get_map_id_from_scene_id(scene_id)\n",
                "print(f'文件路径: {test_path}')\n",
                "print(f'提取的 scene_id: {scene_id}')\n",
                "print(f'对应的 map_id: {map_id}')\n",
                "print(f'地图信息: {map_scene[str(map_id)]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c74a94c6",
            "metadata": {},
            "source": [
                "#### 对于局部池取stock\n",
                "* \n",
                "* 去D:\\fishinggame\\precompute\\data\\1\\1001\\fish_stock.json当中，"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "326ea7f4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "已加载 7 个鱼塘配置\n",
                        "已加载 6 个 Stock 配置\n",
                        "\n",
                        "当前地图: map_base_6 (ID: 1009)\n",
                        "关联的 Desc ID (用于对应 fish_pond_list.mapId): 106\n",
                        "\n",
                        "查找 mapId == 106 的鱼塘...\n",
                        "找到 1 个关联鱼塘:\n",
                        "  - Pond: Sunset_Stream (ID: 301020005) -> Stock ID: 301030106\n",
                        "    Stock 详情: Name=stock_sunset, ResetTime=05:00\n"
                    ]
                }
            ],
            "source": [
                "# 加载额外的配置表\n",
                "with open(DATA_ROOT / 'fish_pond_list.json', 'r', encoding='utf-8') as f:\n",
                "    fish_pond_list = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'fish_stock.json', 'r', encoding='utf-8') as f:\n",
                "    fish_stock_config = json.load(f)\n",
                "\n",
                "print(f\"已加载 {len(fish_pond_list)} 个鱼塘配置\")\n",
                "print(f\"已加载 {len(fish_stock_config)} 个 Stock 配置\")\n",
                "\n",
                "# 获取 map_id 对应的 map_scene 配置\n",
                "current_map_info = map_scene.get(str(map_id))\n",
                "if not current_map_info:\n",
                "    raise ValueError(f\"Found no map info for id {map_id}\")\n",
                "\n",
                "map_desc_id = current_map_info.get('desc')\n",
                "print(f\"\\n当前地图: {current_map_info['name']} (ID: {map_id})\")\n",
                "print(f\"关联的 Desc ID (用于对应 fish_pond_list.mapId): {map_desc_id}\")\n",
                "\n",
                "# 查找关联的 Pond 和 Stock\n",
                "print(f\"\\n查找 mapId == {map_desc_id} 的鱼塘...\")\n",
                "related_ponds = [pond for pond in fish_pond_list.values() if pond.get('mapId') == map_desc_id]\n",
                "\n",
                "if not related_ponds:\n",
                "    print(\"警告: 未找到关联的鱼塘配置 (Pond)\")\n",
                "else:\n",
                "    print(f\"找到 {len(related_ponds)} 个关联鱼塘:\")\n",
                "    for pond in related_ponds:\n",
                "        # 兼容处理: 有些json key可能是str类型的id\n",
                "        pond_id = pond.get('id')\n",
                "        pond_name = pond.get('name')\n",
                "        stock_id = pond.get('fishStockId')\n",
                "        \n",
                "        print(f\"  - Pond: {pond_name} (ID: {pond_id}) -> Stock ID: {stock_id}\")\n",
                "        \n",
                "        # 查询 Stock 详情 (注意 key 可能是字符串)\n",
                "        stock_info = fish_stock_config.get(str(stock_id))\n",
                "        if stock_info:\n",
                "             print(f\"    Stock 详情: Name={stock_info.get('name')}, ResetTime={stock_info.get('resetDayTime')}\")\n",
                "        else:\n",
                "             print(f\"    警告: 在 fish_stock.json 中未找到 Stock ID {stock_id}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "38a062d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 顺着往下进行数据查找和组装numpy，供后面的计算使用。\n",
                "# 大致思路为：\n",
                "# 5. 遍历 Stock ID (Stock -> Release)：\n",
                "#    - 从相关联的池塘中提取 Stock ID，查找其下属的所有 Release ID。\n",
                "# 6. Eager Loading (Release -> Fish/Env):\n",
                "#    - 对每个 Release ID，立即提取所需的全部配置信息，包括：\n",
                "#      - 基础属性: qualityId (即原 fishId), weight/length ranges (min/max).\n",
                "#      - 关键系数: minEnvCoeff, minAdaptCoeff.\n",
                "#      - 关联元数据: speciesId, envAffinityId (即原 envId).\n",
                "# 7. 组装 DataFrame (Assembly):\n",
                "#    - 将上述所有提取的字段扁平化，组装成 Pandas DataFrame (`stockFishesPd`)。\n",
                "#    - 每一行代表一个 Release 配置，为后续的概率计算和环境场生成做准备。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "bf363e58",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Release and Quality configs...\n",
                        "Configs loaded.\n",
                        "Processing 1 unique Stock IDs associated with the current map.\n",
                        "Created stockFishesPd with 35 rows.\n",
                        "     stockId  releaseId  qualityId  envAffinityId  speciesId  weight_min  weight_max  len_min  len_max  minEnvCoeff  minAdaptCoeff                                name  probWeight\n",
                        "0  301030106     300500  101034430        1013390  101020063         150         450       26       37            0              0  Release_American_Shad_Young_sunset      250000\n",
                        "1  301030106     300510  101034090        1013050  101020010          50         200       16       26            0              0    Release_Brook_Trout_Young_sunset      100000\n",
                        "2  301030106     300520  101031007        1010066  101020010         200         350       26       32            0              0   Release_Brook_Trout_Common_sunset      100000\n",
                        "3  301030106     300530  101034450        1013410  101020003         150         450       28       40            0              0         Release_Bowfin_Young_sunset      200000\n",
                        "4  301030106     300540  101034510        1013470  101020050         550         700       37       40            0              0       Release_Hardhead_Young_sunset       17928\n",
                        "\n",
                        "Column Types:\n",
                        "stockId           int64\n",
                        "releaseId         int64\n",
                        "qualityId         int64\n",
                        "envAffinityId     int64\n",
                        "speciesId         int64\n",
                        "weight_min        int64\n",
                        "weight_max        int64\n",
                        "len_min           int64\n",
                        "len_max           int64\n",
                        "minEnvCoeff       int64\n",
                        "minAdaptCoeff     int64\n",
                        "name             object\n",
                        "probWeight        int64\n",
                        "dtype: object\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Load additional configurations\n",
                "print(\"Loading Release and Quality configs...\")\n",
                "with open(DATA_ROOT / 'stock_release.json', 'r', encoding='utf-8') as f:\n",
                "    stock_release_config = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'fish_release.json', 'r', encoding='utf-8') as f:\n",
                "    fish_release_config = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'basic_fish_quality.json', 'r', encoding='utf-8') as f:\n",
                "    basic_fish_quality_config = json.load(f)\n",
                "print(\"Configs loaded.\")\n",
                "\n",
                "rows = []\n",
                "\n",
                "# 'related_ponds' should be available from the previous cell execution\n",
                "# If not, we rely on the logic that this cell is run after Cell 5.\n",
                "if 'related_ponds' not in locals():\n",
                "    print(\"Warning: 'related_ponds' not found. Please ensure the previous cell is executed.\")\n",
                "    unique_stock_ids = set()\n",
                "else:\n",
                "    unique_stock_ids = set(pond.get('fishStockId') for pond in related_ponds if pond.get('fishStockId'))\n",
                "\n",
                "print(f\"Processing {len(unique_stock_ids)} unique Stock IDs associated with the current map.\")\n",
                "\n",
                "for stock_id in unique_stock_ids:\n",
                "    # Find all releases for this stock\n",
                "    # Note: Scanning all values in stock_release_config might be inefficient for very large datasets,\n",
                "    # but acceptable for this precompute scope.\n",
                "    stock_releases = [item for item in stock_release_config.values() if item.get('stockId') == stock_id]\n",
                "    \n",
                "    for sr in stock_releases:\n",
                "        release_id = sr.get('releaseId')\n",
                "        fish_quality_id = sr.get('fishId') # referred as fishId in stock_release.json, but actually qualityId\n",
                "        fish_env_affinity_id = sr.get('fishEnvId') # referred as fishEnvId in stock_release.json\n",
                "        \n",
                "        # Lookup Release Info\n",
                "        release_info = fish_release_config.get(str(release_id))\n",
                "        if not release_info:\n",
                "            # print(f\"Warning: Release ID {release_id} not found in fish_release.json\")\n",
                "            continue\n",
                "            \n",
                "        # Lookup Fish Quality Info\n",
                "        fish_info = basic_fish_quality_config.get(str(fish_quality_id))\n",
                "        species_id = fish_info.get('species', -1) if fish_info else -1\n",
                "            \n",
                "        row = {\n",
                "            'stockId': stock_id,\n",
                "            'releaseId': release_id,\n",
                "            'qualityId': fish_quality_id,\n",
                "            'envAffinityId': fish_env_affinity_id, # Renamed from envId for clarity\n",
                "            'speciesId': species_id,\n",
                "            \n",
                "            # Release Limits\n",
                "            'weight_min': release_info.get('weightMin'),\n",
                "            'weight_max': release_info.get('weightMax'),\n",
                "            'len_min': release_info.get('lengthMin'),\n",
                "            'len_max': release_info.get('lengthMax'),\n",
                "\n",
                "            # Environment Coefficients (Added per request)\n",
                "            'minEnvCoeff': release_info.get('minEnvCoeff', 0),\n",
                "            'minAdaptCoeff': release_info.get('minAdaptCoeff', 0),\n",
                "            \n",
                "            # Debug/Display info\n",
                "            'name': release_info.get('name'),\n",
                "            'probWeight': release_info.get('probWeightIdeal')\n",
                "        }\n",
                "        rows.append(row)\n",
                "\n",
                "stockFishesPd = pd.DataFrame(rows)\n",
                "print(f\"Created stockFishesPd with {len(stockFishesPd)} rows.\")\n",
                "if not stockFishesPd.empty:\n",
                "    print(stockFishesPd.head().to_string())\n",
                "    print(\"\\nColumn Types:\")\n",
                "    print(stockFishesPd.dtypes)\n",
                "else:\n",
                "    print(\"DataFrame is empty. Check if stock_release.json maps correctly to the pond stock IDs.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "87e08507",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 继续进行后续数据关联 (Data Enrichment Phase II)\n",
                "\n",
                "# 8. 环境亲和性关联 (Environment Affinity Lookup):\n",
                "#    - 目标: 丰富鱼类的环境适应参数。\n",
                "#    - 操作: 使用 `envAffinityId` (原 `fishEnvId`) 关联 `fish_env_affinity.json`。\n",
                "#    - 提取关键属性 (Attributes Extraction):\n",
                "#         - 基础ID关联: structId (结构), tempId (温度), layerId (水层), lightId (光照)。\n",
                "#         - 诱鱼系数: baitCoeffGroup, baitTypeCoeffGroup, periodCoeffGroup (时段)。\n",
                "#         - 适应性参数: \n",
                "#             - pressureSensitivity (气压敏感度)\n",
                "#             - minAdaptLureRatio / maxAdaptLureRatio (路亚适应比例)\n",
                "#             - maxAcceptLengthRatio (最大接受长度比)\n",
                "#         - 衰减配置: underLengthDecayCoeff / overLengthDecayCoeff (体型偏离衰减)。\n",
                "\n",
                "# 9. 结构体亲和性级联查找 (Structure Affinity Cascade):\n",
                "#    - 目标: 获取具体的物理结构交互参数。\n",
                "#    - 操作: 使用步骤 8 获得的 `structId`，查询 `struct_affinity.json`。\n",
                "#    - 提取参数 (Parameters Extraction): \n",
                "#         - `List`: 包含 `structType` (结构类型) 和 `coeff` (系数) 的列表。\n",
                "\n",
                "# 10. 温度亲和性级联查找 (Temperature Affinity Cascade):\n",
                "#    - 目标: 获取鱼类对温度的敏感度配置。\n",
                "#    - 操作: 使用步骤 8 获得的 `tempId`，查询 `temp_affinity.json`。\n",
                "#    - 提取参数 (Parameters Extraction): \n",
                "#         - `temperatureFav`: 最适温度 (注意可能需要缩放，如 220 -> 22.0)。\n",
                "#         - `tempAffectedRatio`: 温度影响比率。\n",
                "#         - `tempThreshold`: 温度容忍阈值。\n",
                "\n",
                "# 11. 水层亲和性级联查找 (Water Layer Affinity Cascade):\n",
                "#    - 目标: 获取鱼类在不同水层的分布偏好。\n",
                "#    - 操作: 使用步骤 8 获得的 `layerId`，查询 `water_layer_affinity.json`。\n",
                "#    - 提取参数 (Parameters Extraction): \n",
                "#         - `List`: 包含 `layerType` (水层类型, 如上/中/下) 和 `coeff` (系数) 的列表。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "992238dc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Affinity Configs...\n",
                        "Affinity Configs loaded.\n",
                        "Enriching DataFrame...\n",
                        "Enrichment Complete.\n",
                        "New DataFrame Shape: (35, 31)\n",
                        "   qualityId  envAffinityId  temperatureFav  structId   tempId                                                                                   layerList  baitCoeffGroup  baitTypeCoeffGroup  periodCoeffGroup\n",
                        "0  101034430        1013390             195   2011030  2021010  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]           80000                3105              2011\n",
                        "1  101034090        1013050             195   2010940  2020920  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]           80000                3096              2011\n",
                        "2  101031007        1010066             195   2010940  2020920  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]           80000                3096              2011\n",
                        "3  101034450        1013410             195   2011040  2021020  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]           80000                3106              2011\n",
                        "4  101034510        1013470             220   2011060  2021040  [{'layerType': 1, 'coeff': 1}, {'layerType': 2, 'coeff': 1}, {'layerType': 3, 'coeff': 1}]           80000                3108              2011\n"
                    ]
                }
            ],
            "source": [
                "# 9-11. 实现环境与亲和性级联查找 (Env & Affinity Cascade Lookup)\n",
                "\n",
                "print(\"Loading Affinity Configs...\")\n",
                "# 1. 加载所有亲和性配置 (Load Configs)\n",
                "with open(DATA_ROOT / 'fish_env_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    env_affinity_config = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'struct_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    struct_affinity_config = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'temp_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    temp_affinity_config = json.load(f)\n",
                "\n",
                "with open(DATA_ROOT / 'water_layer_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    layer_affinity_config = json.load(f)\n",
                "print(\"Affinity Configs loaded.\")\n",
                "\n",
                "# 2. 准备查找字典 (Prepare Lookup Dicts)\n",
                "#    优化: 直接构建 id -> data 的快速查找字典，避免每次遍历 list\n",
                "#    注意: JSON key通常是字符串, DataFrame中Id可能是int, 查找时需注意类型转换\n",
                "\n",
                "def get_config_by_id(config_dict, target_id):\n",
                "    \"\"\"Safe lookup helper handling str/int key mismatch\"\"\"\n",
                "    if target_id is None:\n",
                "        return None\n",
                "    return config_dict.get(str(target_id))\n",
                "\n",
                "# 3. 扩展 DataFrame (Enrich DataFrame)\n",
                "#    虽然可以使用 apply，但在列数较多且逻辑复杂时，迭代或列表推导式便于调试和错误处理\n",
                "#    考虑到数据量不大 (几十到几百行)，直接遍历 row 更新字典列表然后重新创建 DF 也是一种清晰的方法\n",
                "#    或者使用 apply + Series expand\n",
                "\n",
                "def enrich_row(row):\n",
                "    # Step 8: Env Affinity Lookup\n",
                "    env_id = row.get('envAffinityId')\n",
                "    env_info = get_config_by_id(env_affinity_config, env_id)\n",
                "    \n",
                "    extra_data = {}\n",
                "    \n",
                "    if env_info:\n",
                "        # Extract basic Env IDs\n",
                "        struct_id = env_info.get('structId')\n",
                "        temp_id = env_info.get('tempId')\n",
                "        layer_id = env_info.get('layerId')\n",
                "        light_id = env_info.get('lightId')\n",
                "        \n",
                "        extra_data.update({\n",
                "            'structId': struct_id,\n",
                "            'tempId': temp_id,\n",
                "            'layerId': layer_id,\n",
                "            'lightId': light_id,\n",
                "            # Coeffs\n",
                "            'baitCoeffGroup': env_info.get('baitCoeffGroup'),\n",
                "            'baitTypeCoeffGroup': env_info.get('baitTypeCoeffGroup'),\n",
                "            'periodCoeffGroup': env_info.get('periodCoeffGroup'),\n",
                "            # Adaptability Stats\n",
                "            'pressureSensitivity': env_info.get('pressureSensitivity'),\n",
                "            'minAdaptLureRatio': env_info.get('minAdaptLureRatio'),\n",
                "            'maxAdaptLureRatio': env_info.get('maxAdaptLureRatio'),\n",
                "            'maxAcceptLengthRatio': env_info.get('maxAcceptLengthRatio'),\n",
                "            'underLengthDecayCoeff': env_info.get('underLengthDecayCoeff'),\n",
                "            'overLengthDecayCoeff': env_info.get('overLengthDecayCoeff'),\n",
                "        })\n",
                "        \n",
                "        # Step 9: Structure Affinity Cascade\n",
                "        struct_info = get_config_by_id(struct_affinity_config, struct_id)\n",
                "        if struct_info:\n",
                "            extra_data['structList'] = struct_info.get('List') # raw list of {structType, coeff}\n",
                "        \n",
                "        # Step 10: Temperature Affinity Cascade\n",
                "        temp_info = get_config_by_id(temp_affinity_config, temp_id)\n",
                "        if temp_info:\n",
                "            extra_data['temperatureFav'] = temp_info.get('temperatureFav')\n",
                "            extra_data['tempAffectedRatio'] = temp_info.get('tempAffectedRatio')\n",
                "            extra_data['tempThreshold'] = temp_info.get('tempThreshold')\n",
                "            \n",
                "        # Step 11: Water Layer Affinity Cascade\n",
                "        layer_info = get_config_by_id(layer_affinity_config, layer_id)\n",
                "        if layer_info:\n",
                "            extra_data['layerList'] = layer_info.get('List') # raw list of {layerType, coeff}\n",
                "            \n",
                "    return pd.Series(extra_data)\n",
                "\n",
                "# 应用扩展逻辑\n",
                "print(\"Enriching DataFrame...\")\n",
                "if not stockFishesPd.empty:\n",
                "    enriched_columns = stockFishesPd.apply(enrich_row, axis=1)\n",
                "    \n",
                "    # Concatenate original df with new columns\n",
                "    stockFishesPd = pd.concat([stockFishesPd, enriched_columns], axis=1)\n",
                "    \n",
                "    print(\"Enrichment Complete.\")\n",
                "    print(f\"New DataFrame Shape: {stockFishesPd.shape}\")\n",
                "    print(stockFishesPd[['qualityId','envAffinityId','temperatureFav', 'structId', 'tempId', 'layerList', 'baitCoeffGroup', 'baitTypeCoeffGroup', 'periodCoeffGroup']].head().to_string())\n",
                "else:\n",
                "    print(\"stockFishesPd is empty, skipping enrichment.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a9b54414",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Bait/Period Configs...\n",
                        "Aggregating Groups...\n",
                        "Enriching StockFishesPd with Bait/Period lists...\n",
                        "Enrichment Complete.\n",
                        "DataFrame Shape: (35, 34)\n",
                        "Sample Row 0 - QualityID: 101034430\n",
                        "BaitCoeffGroup: 80000\n",
                        "BaitList Count: 1\n",
                        "PeriodList Count: 8\n"
                    ]
                }
            ],
            "source": [
                "# 12-14. 诱鱼与时段亲和性关联 (Bait & Period Affinity)\n",
                "\n",
                "import json\n",
                "import pandas as pd\n",
                "\n",
                "print(\"Loading Bait/Period Configs...\")\n",
                "# Load JSONs\n",
                "with open(DATA_ROOT / 'bait_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    bait_affinity_data = json.load(f)\n",
                "with open(DATA_ROOT / 'bait_type_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    bait_type_affinity_data = json.load(f)\n",
                "with open(DATA_ROOT / 'period_affinity.json', 'r', encoding='utf-8') as f:\n",
                "    period_affinity_data = json.load(f)\n",
                "\n",
                "# Helper to aggregate by group\n",
                "# Transform {id: {data}} -> {group_id: [data_list]}\n",
                "def aggregate_by_group(source_data, group_key_name):\n",
                "    grouped = {}\n",
                "    for item in source_data.values():\n",
                "        grp_id = item.get(group_key_name)\n",
                "        \n",
                "        # Safe cast to string for consistent lookup key\n",
                "        if grp_id is not None:\n",
                "            grp_key = str(int(grp_id)) # int -> str to match potential int IDs\n",
                "            if grp_key not in grouped:\n",
                "                grouped[grp_key] = []\n",
                "            grouped[grp_key].append(item)\n",
                "    return grouped\n",
                "\n",
                "print(\"Aggregating Groups...\")\n",
                "# Note: 'baitCoeffGroup' matches the field in fish_env_affinity\n",
                "bait_groups = aggregate_by_group(bait_affinity_data, 'baitCoeffGroup') \n",
                "bait_type_groups = aggregate_by_group(bait_type_affinity_data, 'baitTypeCoeffGroup') \n",
                "\n",
                "# Note: In period_affinity.json, the key is 'periodGroup', but in fish_env it is 'periodCoeffGroup'\n",
                "period_groups = aggregate_by_group(period_affinity_data, 'periodGroup')\n",
                "\n",
                "# Enrich Wrapper\n",
                "def enrich_bait_period(row):\n",
                "    # Lookup Bait Group\n",
                "    # row['baitCoeffGroup'] comes from fish_env_affinity, expected to be int or str\n",
                "    b_val = row.get('baitCoeffGroup')\n",
                "    if pd.notnull(b_val):\n",
                "        b_grp = str(int(b_val))\n",
                "        bait_list = bait_groups.get(b_grp, [])\n",
                "    else:\n",
                "        bait_list = []\n",
                "    \n",
                "    # Lookup Bait Type Group\n",
                "    bt_val = row.get('baitTypeCoeffGroup')\n",
                "    if pd.notnull(bt_val):\n",
                "        bt_grp = str(int(bt_val))\n",
                "        bait_type_list = bait_type_groups.get(bt_grp, [])\n",
                "    else:\n",
                "        bait_type_list = []\n",
                "    \n",
                "    # Lookup Period Group\n",
                "    p_val = row.get('periodCoeffGroup')\n",
                "    if pd.notnull(p_val):\n",
                "        p_grp = str(int(p_val))\n",
                "        period_list = period_groups.get(p_grp, [])\n",
                "    else:\n",
                "        period_list = []\n",
                "    \n",
                "    return pd.Series({\n",
                "        'baitList': bait_list,          # detailed list of {baitId, coeff...}\n",
                "        'baitTypeList': bait_type_list, # detailed list of {baitSubType, coeff...}\n",
                "        'periodList': period_list       # detailed list of {periodId, activityFactor...}\n",
                "    })\n",
                "\n",
                "print(\"Enriching StockFishesPd with Bait/Period lists...\")\n",
                "if not stockFishesPd.empty:\n",
                "    bp_columns = stockFishesPd.apply(enrich_bait_period, axis=1)\n",
                "    \n",
                "    # Concatenate\n",
                "    # Drop existing if re-running to avoid dupe columns\n",
                "    cols_to_drop = [c for c in ['baitList', 'baitTypeList', 'periodList'] if c in stockFishesPd.columns]\n",
                "    if cols_to_drop:\n",
                "         stockFishesPd = stockFishesPd.drop(columns=cols_to_drop)\n",
                "            \n",
                "    stockFishesPd = pd.concat([stockFishesPd, bp_columns], axis=1)\n",
                "    \n",
                "    print(\"Enrichment Complete.\")\n",
                "    print(f\"DataFrame Shape: {stockFishesPd.shape}\")\n",
                "    \n",
                "    # Sample Output\n",
                "    # Only show limited info\n",
                "    cols_check = ['qualityId', 'baitCoeffGroup', 'baitList', 'periodList']\n",
                "    # Just print the first row nicely formatted\n",
                "    first_row = stockFishesPd.iloc[0]\n",
                "    print(f\"Sample Row 0 - QualityID: {first_row['qualityId']}\")\n",
                "    print(f\"BaitCoeffGroup: {first_row['baitCoeffGroup']}\")\n",
                "    print(f\"BaitList Count: {len(first_row['baitList'])}\")\n",
                "    print(f\"PeriodList Count: {len(first_row['periodList'])}\")\n",
                "else:\n",
                "    print(\"DataFrame empty.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "matrix_conversion_title",
            "metadata": {},
            "source": [
                "### 核心计算准备 1：数据矩阵化 (Matrix Conversion)\n",
                "将 DataFrame 中的嵌套 List 转换为稠密 Numpy 矩阵，以便于并行查找和计算。\n",
                "\n",
                "*   **StructAffinityMatrix**: `[NumFishes, MaxStructTypes]`\n",
                "*   **LayerAffinityMatrix**: `[NumFishes, MaxLayerTypes]`\n",
                "*   **TempAffinityParams**: `[NumFishes, 3]` (Fav, Ratio, Threshold)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "matrix_conversion_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# 1. 定义常量\n",
                "# 假设系统中的结构类型和水层类型最大数量 (根据策划文档或数据扫描确定)\n",
                "MAX_STRUCT_TYPES = 20  # structId 范围通常在一定区间，这里假设映射到0-19或直接用TypeID索引，需确认TypeID最大值。\n",
                "# 实际上 StructType 是枚举值，建议先扫描一遍所有出现的 StructType 建立映射，或者直接用 TypeID 作为索引 (如果 ID 较小)。\n",
                "# 观察数据 structList: [{'structType': 1, 'coeff': 100}, ...]\n",
                "# 假设 structType 是从 1 开始的小整数。\n",
                "MAX_STRUCT_TYPE_ID = 30 # 安全起见给大一点\n",
                "\n",
                "MAX_LAYER_TYPES = 5 # 上中下底等，通常<5\n",
                "\n",
                "\n",
                "def build_dense_matrices(df):\n",
                "    num_fishes = len(df)\n",
                "    \n",
                "    # --- A. Struct Affinity Matrix ---\n",
                "    # Shape: [F, S], Init with 0.0 (Assuming 0 affinity if not listed)\n",
                "    struct_matrix = np.zeros((num_fishes, MAX_STRUCT_TYPE_ID), dtype=np.float16)\n",
                "    \n",
                "    # --- B. Layer Affinity Matrix ---\n",
                "    # Shape: [F, L]\n",
                "    layer_matrix = np.zeros((num_fishes, MAX_LAYER_TYPES), dtype=np.float16)\n",
                "    \n",
                "    # --- C. Temp Affinity Params ---\n",
                "    # [Fav, Ratio, Threshold]\n",
                "    temp_params = np.zeros((num_fishes, 3), dtype=np.float16)\n",
                "    \n",
                "    # Fill Data\n",
                "    for idx, row in df.iterrows():\n",
                "        # 1. Struct\n",
                "        # row['structList'] is list of dict or NaN\n",
                "        s_list = row.get('structList')\n",
                "        if isinstance(s_list, list):\n",
                "            for item in s_list:\n",
                "                s_type = item.get('structType')\n",
                "                coeff = item.get('coeff')\n",
                "                if s_type is not None and s_type < MAX_STRUCT_TYPE_ID:\n",
                "                    struct_matrix[idx, s_type] = coeff\n",
                "        else:\n",
                "            # If no struct preference, it might mean insensitive (100) or avoid (0).\n",
                "            # Assuming insensitive (1.0 = 100) for now if not specified.\n",
                "            struct_matrix[idx, :] = 100 \n",
                "\n",
                "        # 2. Layer\n",
                "        l_list = row.get('layerList')\n",
                "        if isinstance(l_list, list):\n",
                "            for item in l_list:\n",
                "                l_type = item.get('layerType')\n",
                "                coeff = item.get('coeff')\n",
                "                if l_type is not None and l_type < MAX_LAYER_TYPES:\n",
                "                    layer_matrix[idx, l_type] = coeff\n",
                "        else:\n",
                "             # Default 1.0 if no layer pref? \n",
                "             layer_matrix[idx, :] = 1.0\n",
                "        \n",
                "        # 3. Temp\n",
                "        # Params: temperatureFav, tempAffectedRatio, tempThreshold\n",
                "        t_fav = row.get('temperatureFav')\n",
                "        t_ratio = row.get('tempAffectedRatio')\n",
                "        t_thres = row.get('tempThreshold')\n",
                "        \n",
                "        if pd.notnull(t_fav):\n",
                "             temp_params[idx, 0] = t_fav / 10.0 # 220 -> 22.0\n",
                "        else:\n",
                "             # Error Out as per decision\n",
                "             # raise ValueError(f\"Missing temperatureFav for fish index {idx}\")\n",
                "             pass\n",
                "             \n",
                "        if pd.notnull(t_ratio):\n",
                "             temp_params[idx, 1] = t_ratio / 10000.0\n",
                "             \n",
                "        if pd.notnull(t_thres):\n",
                "             temp_params[idx, 2] = t_thres / 10000.0\n",
                "\n",
                "    return struct_matrix, layer_matrix, temp_params\n",
                "\n",
                "print(\"Converting to Dense Matrices...\")\n",
                "# Ensure strict ordering by resetting index\n",
                "stockFishesPd.reset_index(drop=True, inplace=True)\n",
                "m_struct, m_layer, m_temp = build_dense_matrices(stockFishesPd)\n",
                "\n",
                "print(f\"Struct Matrix: {m_struct.shape}, dtype={m_struct.dtype}\")\n",
                "print(f\"Layer Matrix:  {m_layer.shape}, dtype={m_layer.dtype}\")\n",
                "print(f\"Temp Params:   {m_temp.shape}, dtype={m_temp.dtype}\")\n",
                "\n",
                "print(f\"Max Struct Val: {np.max(m_struct)}\")\n",
                "print(f\"Max Layer Val: {np.max(m_layer)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "voxel_loading_title",
            "metadata": {},
            "source": [
                "### 核心计算准备 2：加载体素数据 (Load Voxel Data)\n",
                "读取 `Global.npy`，提取 Structural Slots 和 Depth Layer 信息。\n",
                "\n",
                "*   **Data**: `[X, Z, C]`\n",
                "*   **Target**: `struct_slots [X, Z, 3]`, `depth_layer [X, Z]` (+ Y extension)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "voxel_loading_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Voxel Data\n",
                "# test_path comes from previous cells: D:\\fishinggame\\ExportedData\\...\\Fishing_1006001_Global.npy\n",
                "print(f\"Loading Voxel Data from: {test_path}\")\n",
                "voxel_data = np.load(test_path)\n",
                "print(f\"Loaded Voxel Data Shape: {voxel_data.shape}, dtype={voxel_data.dtype}\")\n",
                "\n",
                "# Parse based on VoxelMapDataFormat.md (Simplified Assumptions for Demo)\n",
                "# Assuming:\n",
                "# Channel 0: Struct Layer 1\n",
                "# Channel 1: Struct Layer 2\n",
                "# Channel 2: Struct Layer 3\n",
                "# Channel 3: Depth/Terrain info ...\n",
                "# We need to check the mask definition in C# or MD.\n",
                "# For this demo, let's assume Channels 0,1,2 contain StructTypeIds (or -1).\n",
                "# Note: In .npy from Unity, usually it's [X, Z, Channels]\n",
                "\n",
                "# Demo: Slice struct slots\n",
                "struct_slots_map = voxel_data[:, :, 0:3].astype(np.int32)\n",
                "# If data is float or other bits, might need decoding. Assuming direct ID for now.\n",
                "\n",
                "# Demo: Slice depth info (Assuming Channel 3 is Depth/Layer)\n",
                "# In actual format, might be a bitmask. Let's assume it's normalized depth 0-1 or layer index.\n",
                "depth_map_raw = voxel_data[:, :, 3]\n",
                "\n",
                "print(f\"Struct Slots Map Shape: {struct_slots_map.shape}\")\n",
                "print(f\"Depth Raw Map Shape: {depth_map_raw.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "core_calc_title",
            "metadata": {},
            "source": [
                "### 核心计算 3：批量亲和度计算 (Data-Driven Batch Calculation)\n",
                "\n",
                "1.  **AffStruct**: `StructMatrix` vs `StructSlotsMap` (Gather)\n",
                "2.  **AffTemp**: `TempParams` vs `DepthMap` (Gaussian)\n",
                "3.  **Synthesis**: `EnvCoeff`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "core_calc_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# -----------------------------------------------------\n",
                "# A. Struct Affinity Calculation (Simplified 2D for Demo)\n",
                "# -----------------------------------------------------\n",
                "# Goal: affStruct[x, z, f] = Max over 3 slots (StructMatrix[f, SlotId])\n",
                "\n",
                "# Input: \n",
                "#   struct_slots_map: [X, Z, 3]\n",
                "#   m_struct: [F, S]\n",
                "\n",
                "# Dimensions\n",
                "X, Z, _ = struct_slots_map.shape\n",
                "F = m_struct.shape[0]\n",
                "\n",
                "print(f\"Calculating Struct Affinity for Grid [{X}x{Z}] and {F} Fish...\")\n",
                "\n",
                "# Naive Broadcasting Approach (Memory Heavy for huge maps, ok for Demo)\n",
                "# Advanced: Use Numba for loop-fusion.\n",
                "\n",
                "# 1. Flatten slots to facilitate indexing\n",
                "# [X*Z, 3]\n",
                "slots_flat = struct_slots_map.reshape(-1, 3)\n",
                "\n",
                "# 2. Gather values from m_struct\n",
                "# We want result: [X*Z, 3, F]\n",
                "# StructMatrix is [F, S]. \n",
                "# We effectively want m_struct[:, slots_flat]\n",
                "# But advanced indexing is tricky with mixed dims.\n",
                "\n",
                "# Let's flip m_struct to [S, F] for easier gathering?\n",
                "m_struct_T = m_struct.T # [S, F]\n",
                "\n",
                "# Safe Indexing: Clip negative indices (-1) to 0 or handle them.\n",
                "# Assume 0 is \"No Struct\" and has affinity 1.0 or 0.0.\n",
                "# Let's clip to 0.\n",
                "slots_safe = np.maximum(slots_flat, 0)\n",
                "\n",
                "# Gather: [X*Z*3, F] -> Reshape [X*Z, 3, F]\n",
                "# result = m_struct_T[slots_safe.flatten()] \n",
                "# shape: [X*Z*3, F]\n",
                "aff_struct_gathered = m_struct_T[slots_safe.flatten()].reshape(X, Z, 3, F)\n",
                "\n",
                "# 3. Max over slots -> [X, Z, F]\n",
                "aff_struct_final = np.max(aff_struct_gathered, axis=2)\n",
                "\n",
                "# Normalize if data was 100-based\n",
                "aff_struct_final = aff_struct_final / 100.0\n",
                "\n",
                "print(f\"AffStruct Final Shape: {aff_struct_final.shape}\")\n",
                "print(\"Sample AffStruct (Fish 0, Grid 0:5,0:5):\")\n",
                "print(aff_struct_final[0:5, 0:5, 0])\n",
                "\n",
                "\n",
                "# -----------------------------------------------------\n",
                "# B. Temp Affinity Calculation (Depth-based)\n",
                "# -----------------------------------------------------\n",
                "# Goal: affTemp[y, f] (Simplified to [y] in demo, actually depth varies)\n",
                "# For Demo, let's assume DepthMap implies a T_val per pixel.\n",
                "# T_pixel = SurfaceT + (BottomT - SurfaceT) * DepthMap (0-1)\n",
                "\n",
                "SURFACE_T = 25.0\n",
                "BOTTOM_T = 10.0\n",
                "\n",
                "# Temp Map [X, Z]\n",
                "t_map = SURFACE_T + (BOTTOM_T - SURFACE_T) * (depth_map_raw / 255.0) # Assuming 8-bit depth\n",
                "\n",
                "# Calc Affinity: exp( - (T - Tfav)^2 / (Width * Ratio^2) )\n",
                "# m_temp: [F, 3] -> Fav, Ratio, Thres\n",
                "\n",
                "# Broadcast T_map to [X, Z, F]\n",
                "t_map_expanded = t_map[:, :, np.newaxis] # [X, Z, 1]\n",
                "\n",
                "t_fav = m_temp[:, 0] # [F]\n",
                "t_ratio = m_temp[:, 1] # [F]\n",
                "# Width constant\n",
                "WIDTH_CONST = 50.0 # From guide/config\n",
                "\n",
                "# (T - Tfav)^2\n",
                "diff_sq = (t_map_expanded - t_fav) ** 2 # [X, Z, F]\n",
                "\n",
                "# Denom: Width * Ratio^2\n",
                "# Avoid div by zero\n",
                "denom = WIDTH_CONST * (t_ratio ** 2)\n",
                "denom[denom < 1e-5] = 1e-5\n",
                "\n",
                "aff_temp_final = np.exp(- diff_sq / denom)\n",
                "\n",
                "print(f\"AffTemp Final Shape: {aff_temp_final.shape}\")\n",
                "print(\"Sample AffTemp (Fish 0, Grid 0:5,0:5):\")\n",
                "print(aff_temp_final[0:5, 0:5, 0])\n",
                "\n",
                "# -----------------------------------------------------\n",
                "# C. Synthesis\n",
                "# -----------------------------------------------------\n",
                "# EnvCoeff = AffStruct * AffTemp * ...\n",
                "\n",
                "env_coeff_final = aff_struct_final * aff_temp_final\n",
                "\n",
                "print(f\"EnvCoeff Final Shape: {env_coeff_final.shape}\")\n",
                "print(\"Sample EnvCoeff (Fish 0, Grid 0:5,0:5):\")\n",
                "print(env_coeff_final[0:5, 0:5, 0])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "fish",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}